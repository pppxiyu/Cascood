{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geopy as gpy\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7835cd",
   "metadata": {},
   "source": [
    "## Incident data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35091fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawData(address = './data/ambulance/virginiaBeach_ambulance_timeData.csv'):\n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    data['CallDateTime'] = pd.to_datetime(data['Call Date and Time'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "    data['EntryDateTime'] = pd.to_datetime(data['Entry Date and Time'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "    data['DispatchDateTime'] = pd.to_datetime(data['Dispatch Date and Time'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "    data['EnRouteDateTime'] = pd.to_datetime(data['En route Date and Time'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "    data['OnSceneDateTime'] = pd.to_datetime(data['On Scene Date and Time'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "    data['CloseDateTime'] = pd.to_datetime(data['Close Date and Time'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    data['Country'] = 'USA'\n",
    "    data['Address'] = data['Block Address'].str.cat([\n",
    "        pd.Series(', ', index = data.index),\n",
    "        data['City'], \n",
    "        pd.Series(', ', index = data.index),\n",
    "        data['State'], \n",
    "        pd.Series(', ', index = data.index),\n",
    "        data['Country']\n",
    "    ], join=\"left\")\n",
    "\n",
    "    data['DispatchTime'] = (data['DispatchDateTime'] - data['CallDateTime']).astype(\"timedelta64[s]\")\n",
    "    data['EnRouteTime'] = (data['EnRouteDateTime'] - data['CallDateTime']).astype(\"timedelta64[s]\")\n",
    "    data['TravelTime'] = (data['OnSceneDateTime'] - data['EnRouteDateTime']).astype(\"timedelta64[s]\")\n",
    "    data['ResponseTime'] = (data['OnSceneDateTime'] - data['CallDateTime']).astype(\"timedelta64[s]\")\n",
    "    data['HourInDay'] = data['CallDateTime'].dt.hour\n",
    "    data['DayOfWeek'] = data['CallDateTime'].dt.dayofweek\n",
    "    \n",
    "    return data\n",
    "\n",
    "def addOrigin(data, rescueAddress):\n",
    "    rescue = pd.read_csv(rescueAddress) \n",
    "    rescue = gpd.GeoDataFrame(rescue, geometry = gpd.points_from_xy(rescue['lon'], rescue['lat']))\n",
    "    data = data[data['Rescue Squad Number'].isin(rescue.Number.to_list())]\n",
    "    data = data.merge(rescue, how = 'left', left_on = 'Rescue Squad Number', right_on = 'Number')\n",
    "    \n",
    "    data = gpd.GeoDataFrame(data)\n",
    "    data = data.set_index('CallDateTime')\n",
    "    data = data.sort_index()\n",
    "    \n",
    "    data['geometry'] = gpd.GeoSeries(data['geometry'], crs = 'EPSG:4326', index = data.index)\n",
    "    return data\n",
    "\n",
    "def geoCoding(data):\n",
    "    locator = gpy.geocoders.ArcGIS()\n",
    "    geocode = RateLimiter(locator.geocode, min_delay_seconds = 0.1)\n",
    "\n",
    "    data['IncidentFullInfo'] = data['Address'].apply(geocode)\n",
    "    data['IncidentCoor'] = data['IncidentFullInfo'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "    data['IncidentFullInfo'] = data['IncidentFullInfo'].astype(str)\n",
    "    data[['IncidentLat', 'IncidentLon', 'IncidentElevation']] = pd.DataFrame(data['IncidentCoor'].tolist(), index = data.index)\n",
    "    data['IncidentPoint'] = gpd.GeoSeries(gpd.points_from_xy(y = data.IncidentLat, x = data.IncidentLon), index = data.index, crs = \"EPSG:4326\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def organizeData(data):\n",
    "    data['CallDateTime'] = data.index\n",
    "    data = data.reset_index(drop = True)\n",
    "    data = data.loc[:, [\n",
    "        'Call Priority',\n",
    "        'CallDateTime', 'EntryDateTime', 'DispatchDateTime', 'EnRouteDateTime', 'OnSceneDateTime', 'CloseDateTime',\n",
    "        'DispatchTime', 'EnRouteTime', 'TravelTime', 'ResponseTime', 'HourInDay', 'DayOfWeek', \n",
    "        'Rescue Squad Number', 'geometry', \n",
    "        'Address', 'IncidentFullInfo', 'IncidentPoint',] ]\n",
    "    data = data.rename(columns = {\"geometry\": \"RescueSquadPoint\", \n",
    "                                  \"Address\": \"IncidentAddress\", \n",
    "                                  'Rescue Squad Number': 'RescueSquadNumber',\n",
    "                                  'Call Priority': 'CallPriority'})\n",
    "    data.set_geometry(\"IncidentPoint\")\n",
    "    return data\n",
    "\n",
    "def string2Points(column, crs, index):\n",
    "    x = [float(location.replace('POINT (', '').replace(')', '').split(' ')[0]) for location in list(data[column].values)]\n",
    "    y = [float(location.replace('POINT (', '').replace(')', '').split(' ')[1]) for location in list(data[column].values)]\n",
    "    return gpd.GeoSeries(gpd.points_from_xy(x = x, y = y), crs = crs, index = index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a61437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and save data\n",
    "data = rawData('./data/ambulance/virginiaBeach_ambulance_timeData.csv')\n",
    "data = addOrigin(data, './data/rescueTeamLocation/rescueStations.txt')\n",
    "data = data.loc['2015-01-01' : '2015-12-31', :]\n",
    "data = geoCoding(data)\n",
    "data = organizeData(data)\n",
    "data.to_csv('./data/geocoded_saved/20150101-20151231.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348bd4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data and build geoDataFrame\n",
    "data = pd.read_csv('./data/geocoded_saved/20160101-20161015.csv', index_col = 'CallDateTime')\n",
    "data.index = pd.to_datetime(data.index, format = \"%Y-%m-%d %H:%M:%S\")\n",
    "data = gpd.GeoDataFrame(data)\n",
    "data['RescueSquadPoint'] = string2Points('RescueSquadPoint', \"EPSG:4326\", data.index)\n",
    "data['IncidentPoint'] = string2Points('IncidentPoint', \"EPSG:4326\", data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca4230d",
   "metadata": {},
   "source": [
    "# Geographical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a1cc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import rasterio.mask\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import momepy\n",
    "import libpysal\n",
    "import math\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419b86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualzation\n",
    "def showRoadsInundation(inundation):\n",
    "    fig = plt.figure(figsize = (100, 50))\n",
    "    ax = fig.add_subplot()\n",
    "    ax = show(inundation, ax = ax, cmap = 'pink')\n",
    "    roads.plot(ax = ax)\n",
    "    plt.show()\n",
    "\n",
    "def showInundation(inundation):\n",
    "    plt.imshow(inundation.read()[0], cmap = 'hot')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def showBridgesInundation():\n",
    "    fig = plt.figure(figsize = (100, 50))\n",
    "    ax = fig.add_subplot()\n",
    "    ax = show(inundation, ax = ax, cmap = 'pink')\n",
    "    bridges.plot(ax = ax)\n",
    "    plt.show()\n",
    "\n",
    "def showLinesSurfaces_withBounds(roads, roadSurfaces, bounds):\n",
    "    fig = plt.figure(figsize = (100, 50))\n",
    "    ax = fig.add_subplot()\n",
    "    roadsBounded = roads.cx[bounds[0]: bounds[1], bounds[2]: bounds[3]]\n",
    "    roadSurfacesBounded = roadSurfaces.cx[bounds[0]: bounds[1], bounds[2]: bounds[3]]\n",
    "    roadsBounded.plot(ax = ax, color='red')\n",
    "    roadSurfacesBounded.plot(ax = ax)\n",
    "    plt.show()    \n",
    "\n",
    "def showMidpointLineSurface(roads, roadSurfaces):\n",
    "    fig = plt.figure(figsize = (400, 200))\n",
    "    ax = fig.add_subplot()\n",
    "    roads.line.plot(ax = ax, linewidth = .75, zorder = 0)\n",
    "    # roads.midpoint.plot(ax = ax, zorder = 0)\n",
    "    roadSurfaces.geometry.plot(ax = ax, color = 'red', zorder = 0)\n",
    "    plt.show()\n",
    "\n",
    "# utilites\n",
    "def getGlobalBounds(gpd):\n",
    "    # get global bounds of a geopandas df\n",
    "    xmin = gpd.bounds.minx.min()\n",
    "    xmax = gpd.bounds.maxx.max()\n",
    "    ymin = gpd.bounds.miny.min()\n",
    "    ymax = gpd.bounds.maxy.max()    \n",
    "    return xmin, xmax, ymin, ymax\n",
    "\n",
    "def getMiddleBounding(bounds, percent = 0.05):\n",
    "    xmin = bounds[0]\n",
    "    xmax = bounds[1]\n",
    "    ymin = bounds[2]\n",
    "    ymax = bounds[3]\n",
    "    xminNew = xmin + ((xmax - xmin) * ((1 - percent) / 2))\n",
    "    xmaxNew = xminNew + (xmax - xmin) * percent\n",
    "    yminNew = ymin + ((ymax - ymin) * ((1 - percent) / 2))\n",
    "    ymaxNew = yminNew + (ymax - ymin) * percent   \n",
    "    return xminNew, xmaxNew, yminNew, ymaxNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39c5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveDuplicates(joined):\n",
    "    # move duplicates, keep the row with higher width\n",
    "    u, c = np.unique(joined.OBJECTID_left.values, return_counts = True)\n",
    "    duplicates = u[c > 1]\n",
    "    joined_noDuplicates = joined.copy()\n",
    "    for dup in duplicates:\n",
    "        du = joined[joined.OBJECTID_left == dup]\n",
    "        joined_noDuplicates = joined_noDuplicates[joined_noDuplicates.OBJECTID_left != dup]\n",
    "        duOne = du[du.aveWidth == du.aveWidth.max()]\n",
    "        joined_noDuplicates = pd.concat([joined_noDuplicates, duOne])\n",
    "    return joined_noDuplicates.sort_values(by = ['OBJECTID_left'])\n",
    "\n",
    "def createSurface4roads(roads, roadSurfaces):\n",
    "    # USE: create a geoDataFrame containing the column of average width and full polygon (might include multiple road segments) for each road\n",
    "    # spatial join road lines and surfaces\n",
    "    if roads.crs != roadSurfaces.crs:\n",
    "        return 'crs not consistent'\n",
    "    roadSurfaces['aveWidth'] = roadSurfaces.Shapearea / roadSurfaces.Shapelen\n",
    "    roads['midpoint'] = roads.geometry.interpolate(0.5, normalized = True)\n",
    "    roads = roads.set_geometry(\"midpoint\", crs = roadSurfaces.crs)\n",
    "    roads = roads.rename(columns = {\"geometry\": \"line\"})\n",
    "    joined = roads.sjoin(roadSurfaces, how = \"left\", predicate = 'within')\n",
    "    # move duplicates/nan \n",
    "    joined_updated = moveDuplicates(joined)\n",
    "    joined_updated.loc[np.isnan(joined_updated.aveWidth), ['aveWidth']] = joined_updated.aveWidth.mean() # assign width to missing roads\n",
    "    # attach roadSurface polygons\n",
    "    joined_updated['OBJECTID_right'] = joined_updated.OBJECTID_right.astype('Int64')\n",
    "    roadSurfaces_temp = roadSurfaces[['OBJECTID', 'geometry']].rename({'OBJECTID': 'OBJECTID_right', 'geometry': 'surfacePolygon'}, axis = 1)\n",
    "    roadSurfaces_temp.loc[len(roadSurfaces_temp)] = [np.nan, Polygon()]\n",
    "    roadSurfaces_temp.OBJECTID_right = roadSurfaces_temp.OBJECTID_right.astype('Int64')\n",
    "    joined_updated = joined_updated.merge(roadSurfaces_temp, how = 'left', on = 'OBJECTID_right')\n",
    "    joined_updated = joined_updated.set_geometry('surfacePolygon').set_crs(roadSurfaces.crs)\n",
    "    return joined_updated\n",
    "\n",
    "roads = gpd.read_file('./data/roads/Streets.shp')\n",
    "roads = roads.loc[-roads['geometry'].duplicated(), :]\n",
    "roads['OBJECTID'] = list(range(1, len(roads) + 1))\n",
    "roads = roads.reset_index(drop = True)\n",
    "\n",
    "roadSurfaces = gpd.read_file('./data/roads/Road_Surfaces.shp')\n",
    "surfaces4roads = createSurface4roads(roads, roadSurfaces)\n",
    "\n",
    "# make buffer for lines\n",
    "scale = 2.7\n",
    "roads['aveWidth'] = surfaces4roads.aveWidth\n",
    "roads['scaledRadius'] = roads['aveWidth'] / 2 * scale\n",
    "roads['buffers'] = roads.geometry.buffer(roads['scaledRadius'])\n",
    "roads['buffersUnscaled'] = roads.geometry.buffer(roads['aveWidth'] / 2 * 1.5) # may be some errors in raw data, roads look good when multiply by 1.5 \n",
    "roads = roads.rename(columns = {\"geometry\": \"line\"})\n",
    "roads = roads.set_geometry('buffers', crs = roadSurfaces.crs)\n",
    "\n",
    "# intersect buffer with roadSurface polygon\n",
    "roads['surface'] = [road.intersection(surface) if not road.intersection(surface).is_empty else roadUnscaled \\\n",
    "                            for road, surface, roadUnscaled in zip(roads.geometry, surfaces4roads.geometry, roads.buffersUnscaled)]\n",
    "roads = roads.set_geometry('surface', crs = roadSurfaces.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f57c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inundationCutter(inundation, cut, all_touched, invert, addr = './data/inundation/croppedByBridge/croppedByBridge.tif'):\n",
    "    if inundation.crs != cut.crs:\n",
    "        return 'crs not consistent'\n",
    "    # mask the inundation using bridges shp, remove the inundation under bridges\n",
    "    out_array, _ = rasterio.mask.mask(inundation, cut.geometry, all_touched = all_touched, invert = invert)\n",
    "    inundation_cropped = rasterio.open(\n",
    "        addr,\n",
    "        'w+',\n",
    "        **inundation.meta\n",
    "    )\n",
    "    inundation_cropped.write(out_array)\n",
    "    return inundation_cropped\n",
    "\n",
    "def getMaxWaterDepth(roadGeometry, inundation):\n",
    "    # roadGeometry should be series, inundation is raster\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        inundationOnRoad, _ = rasterio.mask.mask(inundation, roadGeometry)\n",
    "        inundationOnRoad = np.where(inundationOnRoad == inundation.nodata, - inundationOnRoad, inundationOnRoad)\n",
    "    return np.max(inundationOnRoad)\n",
    "\n",
    "\n",
    "# determine disruption on road network (take one hour as example)\n",
    "inundation = rasterio.open('./data/inundation/tifData/depth_objID_35.tif')\n",
    "roads_updated_4getInundation = roads.copy().to_crs(str(inundation.crs))\n",
    "inundation_cutByRoads = inundationCutter(inundation, roads_updated_4getInundation, False, False, './data/inundation/croppedByRoads/croppedByRoads.tif')\n",
    "\n",
    "roads['waterDepth'] = roads_updated_4getInundation.loc[:, ['surface']] \\\n",
    "    .apply(lambda x: getMaxWaterDepth(x, inundation_cutByRoads), axis = 1, raw = True).replace(-inundation.nodata, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider bridges in road network (PENDING)\n",
    "# bridges = gpd.read_file('./data/bridges/bridgePolygon.shp').to_crs(str(inundation.crs))\n",
    "# inundation_cropped = inundationCutter(inundation, bridges, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd1c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "def showGraphRoads(roads4graph, graph):\n",
    "    f, ax = plt.subplots(1, 3, figsize = (100, 50), sharex = True, sharey = True)\n",
    "    for i, facet in enumerate(ax):\n",
    "        facet.set_title((\"Streets\", \"Primal graph\", \"Overlay\")[i])\n",
    "        facet.axis(\"off\")\n",
    "\n",
    "    roads4graph.plot(color='#e32e00', ax = ax[0])\n",
    "    nx.draw(graph, {key: [value.x, value.y] for key, value in nx.get_node_attributes(graph, 'midpoint').items()}, ax = ax[1], node_size = 1)\n",
    "    roads4graph.plot(color = '#e32e00', ax = ax[2], zorder = -1)\n",
    "    nx.draw(graph, {key: [value.x, value.y] for key, value in nx.get_node_attributes(graph, 'midpoint').items()}, ax = ax[2], node_size = 1)\n",
    "\n",
    "\n",
    "roads4graph = roads.copy()\n",
    "roads4graph['geometry'] = roads4graph['line'].to_crs(roads4graph.line.crs)\n",
    "roads4graph = roads4graph.set_geometry(\"geometry\")\n",
    "graph = momepy.gdf_to_nx(roads4graph, approach = 'dual', multigraph = False, angles = False)\n",
    "graph = nx.relabel_nodes(graph, nx.get_node_attributes(graph, 'OBJECTID'))\n",
    "for edge in graph.edges():\n",
    "    graph[edge[0]][edge[1]]['weight'] = (graph.nodes[edge[0]]['SHAPElen'] + graph.nodes[edge[1]]['SHAPElen']) / 2\n",
    "# showGraphRoads(roads4graph, graph)\n",
    "\n",
    "# NOTE: the graph is un-directed right now, the logic should be checked if changed to directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f875e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def addPathLen2Graph(graph, rescue, weight, newAttribute_rescueSquad, newAttribute_path):\n",
    "    # some roads are disconnected from all the rescue station even in normal time (as the raw data indicates)\n",
    "    voronoi = nx.voronoi_cells(graph, set(rescue.OBJECTID_nearestRoad.unique()), weight = weight)\n",
    "    for rescueSquad, destinations in zip(voronoi.keys(), voronoi.values()):\n",
    "        if rescueSquad == 'unreachable':\n",
    "            print(len(destinations), 'nodes are unreachable when building voronoi for', newAttribute_path)\n",
    "            for des in destinations:\n",
    "                graph.nodes[des][newAttribute_rescueSquad] = np.nan\n",
    "                graph.nodes[des][newAttribute_path] = math.inf # set path len to inf if it's disconnected from rescues\n",
    "#                 print('NOTE: node', des, 'is unreachable when building voronoi for', newAttribute_path)\n",
    "        else:\n",
    "            for des in destinations:\n",
    "                shortestPath = nx.shortest_path_length(graph, source = rescueSquad, target = des, weight = weight)\n",
    "                graph.nodes[des][newAttribute_path] = shortestPath\n",
    "                graph.nodes[des][newAttribute_rescueSquad] = rescueSquad\n",
    "                if shortestPath == 0:\n",
    "                    graph.nodes[des][newAttribute_path] = 1\n",
    "                if shortestPath == math.inf:\n",
    "                    graph.nodes[des][newAttribute_rescueSquad] = math.inf\n",
    "    return graph, voronoi\n",
    "\n",
    "def addDisruption(graph, roads, newAttribute = 'weightWithDisruption', threshold = 3):\n",
    "    nx.set_edge_attributes(graph, nx.get_edge_attributes(graph, \"weight\"), newAttribute)\n",
    "    disruptedRoads = roads[roads['waterDepth'] >= threshold]['OBJECTID'].to_list()\n",
    "    for disruption in disruptedRoads:\n",
    "        for edge in graph.edges(disruption):\n",
    "            graph.edges()[edge][newAttribute] = math.inf # set edge weight to inf if it's disrupted by inundation\n",
    "    return graph\n",
    "\n",
    "def changeValue4DisruptedRoad(roads, graph, threshold = 3):\n",
    "    # the disrupted road itself is not disconnected, so assign the shortestPath of adjancent road to this road\n",
    "    for disruption in roads[roads['waterDepth'] >= threshold]['OBJECTID'].to_list():\n",
    "        pathLen = []\n",
    "        edgeNum = []\n",
    "        for edge in graph.edges(disruption):\n",
    "            pathLen.append(graph.nodes()[edge[1]]['shortestPathLenWithDisruption'])\n",
    "            edgeNum.append(edge[1])\n",
    "        if pathLen != []: # in case there are disconnected single node\n",
    "            graph.nodes()[disruption]['shortestPathLenWithDisruption'] = min(pathLen)\n",
    "            if min(pathLen) != math.inf:\n",
    "                graph.nodes()[disruption]['rescueAssignedWithDisruption'] = edgeNum[pathLen.index(min(pathLen))]\n",
    "            else:\n",
    "                graph.nodes()[disruption]['rescueAssignedWithDisruption'] = np.nan\n",
    "    return graph\n",
    "\n",
    "\n",
    "# read the location of rescue squads and attach them to nodes \n",
    "rescue = pd.read_csv('./data/rescueTeamLocation/rescueStations.txt') \n",
    "rescue = gpd.GeoDataFrame(rescue, geometry = gpd.points_from_xy(rescue['lon'], rescue['lat'])).set_crs('EPSG:4326').to_crs(roads.crs) \n",
    "rescue['OBJECTID_nearestRoad'] = rescue.geometry.apply(lambda x: x.distance(roads.line).sort_values().index[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a448c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 nodes are unreachable when building voronoi for shortestPathLen\n",
      "81 nodes are unreachable when building voronoi for shortestPathLenWithDisruption\n"
     ]
    }
   ],
   "source": [
    "# calculate ratios\n",
    "graph, _ = addPathLen2Graph(graph, rescue, 'weight', 'rescueAssigned', 'shortestPathLen')\n",
    "graphDisrupted = addDisruption(graph, roads, threshold = 1)\n",
    "graph, _ = addPathLen2Graph(graphDisrupted, rescue, 'weightWithDisruption', 'rescueAssignedWithDisruption', 'shortestPathLenWithDisruption') \n",
    "graph = changeValue4DisruptedRoad(roads, graph, threshold = 1)\n",
    "\n",
    "nx.set_node_attributes(graph, \n",
    "                       {x[0]: y[1]/x[1] if y[1]/x[1] != math.inf else np.nan \\\n",
    "                        for x, y in zip(nx.get_node_attributes(graph, \"shortestPathLen\").items(), \n",
    "                                        nx.get_node_attributes(graph, \"shortestPathLenWithDisruption\").items() ) },\n",
    "                       'travelTimeIncreaseRatio')\n",
    "roads['travelTimeIncreaseRatio'] = roads['OBJECTID'].map(nx.get_node_attributes(graph, \"travelTimeIncreaseRatio\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b6d74",
   "metadata": {},
   "source": [
    "# Incident and geographical info intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "165c150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignGraphEdge(data, roads, inColumn, outColumn1, outColumn2, max_distance = 500):\n",
    "    # NOTE: there could be null value if no road is within the scope of search for a location\n",
    "    roadLines = roads.loc[:, ['OBJECTID', 'line']].set_geometry('line')\n",
    "    locations = data.loc[:, [inColumn]].set_geometry(inColumn).to_crs(roadLines.crs)\n",
    "    match = locations.sjoin_nearest(roadLines, how = 'left', max_distance = max_distance, distance_col = 'distance')\n",
    "    match = match.reset_index().drop_duplicates(subset = ['CallDateTime']).set_index('CallDateTime')\n",
    "    data[outColumn1] = match['OBJECTID']\n",
    "    data[outColumn2] = match['distance']\n",
    "    return data\n",
    "\n",
    "def nearestRescue4Incidents(data, rescue):\n",
    "    # find nearest rescues for all incidents\n",
    "    incidents = data.DestinationID.values\n",
    "    voronoi = nx.voronoi_cells(graph, set(rescue.OBJECTID_nearestRoad.unique()), weight = 'weight')\n",
    "    nearestRescue = []\n",
    "    for incident in incidents:\n",
    "        len1 = len(nearestRescue)\n",
    "        if np.isnan(incident):\n",
    "            nearestRescue.append(np.nan)\n",
    "        else:\n",
    "            for key, value in voronoi.items():\n",
    "                if int(incident) in list(value):\n",
    "                    if key == 'unreachable':\n",
    "                        nearestRescue.append(np.nan)\n",
    "                    else:\n",
    "                        nearestRescue.append(key)\n",
    "                    break \n",
    "        len2 = len(nearestRescue)\n",
    "        if len2 == len1:\n",
    "            print(incident, 'not in any')\n",
    "    return nearestRescue\n",
    "\n",
    "def generateDistDf(rescue, graph):\n",
    "    nodeList = range(1, len(list(graph.nodes())) + 1)\n",
    "    df = pd.DataFrame(nodeList, index = nodeList, columns =['NodeNames'])\n",
    "    for res in rescue.values:\n",
    "        resName = res[0]\n",
    "        resRoadNumber = res[-1]\n",
    "        distanceDict = nx.single_source_dijkstra_path_length(graph, resRoadNumber, weight='weight')\n",
    "        orderedResRoadNumber = OrderedDict(sorted(distanceDict.items()))\n",
    "        orderedResRoadNumberDf = pd.DataFrame.from_dict(orderedResRoadNumber, orient = 'index', columns = ['from' + resName])\n",
    "        orderedResRoadNumberDf = orderedResRoadNumberDf.reset_index()\n",
    "        df = df.merge(orderedResRoadNumberDf, how = 'left', left_on = 'NodeNames', right_on = 'index').drop(columns = 'index')\n",
    "    return df\n",
    "\n",
    "def obedianceOfShortestPrinciple(Series, distanceDataFrame):\n",
    "    DestinationID = Series.DestinationID\n",
    "    RescueSquadNumber = Series.RescueSquadNumber\n",
    "    if len(distanceDataFrame[distanceDataFrame.NodeNames == DestinationID]) != 0:\n",
    "        # NOTE:some incidents are not considered because no road around them\n",
    "        allDist = list(np.sort(distanceDataFrame[distanceDataFrame.NodeNames == DestinationID].values[0][1:]))\n",
    "        realDist = distanceDataFrame[distanceDataFrame.NodeNames == DestinationID]['from' + RescueSquadNumber].values[0]\n",
    "        if np.isnan(realDist):\n",
    "            # NOTE: some roads are disconnected even in normal time\n",
    "            realDistRank = np.nan\n",
    "            realDistIncreaseRatio = np.nan\n",
    "        else:\n",
    "            realDistRank = allDist.index(realDist) + 1\n",
    "            if allDist[0] == 0:\n",
    "                # NOTE: in case the incident is just beside the rescue station, set the dist to 1\n",
    "                allDist[0] = 1\n",
    "            realDistIncreaseRatio = realDist / allDist[0]\n",
    "    else:\n",
    "        realDistRank = np.nan\n",
    "        realDistIncreaseRatio = np.nan\n",
    "    return realDistRank, realDistIncreaseRatio\n",
    "\n",
    "def shortestRouteLength_slow(row, graph, ifPrintError = False):\n",
    "    try:\n",
    "        length = nx.dijkstra_path_length(graph, row.OriginRoadID, row.DestinationID, weight = 'weight')\n",
    "    except BaseException as ex:\n",
    "        if ifPrintError == True:\n",
    "            print(ex)\n",
    "        length = np.nan\n",
    "    return length\n",
    "\n",
    "def shortestRouteLength(s, distanceDataFrame):\n",
    "    if np.isnan(s.DestinationID):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return distanceDataFrame[distanceDataFrame.NodeNames == s.DestinationID]['from' + s.RescueSquadNumber].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9689132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign records to graph edges\n",
    "data = assignGraphEdge(data, roads, 'RescueSquadPoint', 'OriginRoadID', 'Origin2RoadDist')\n",
    "data = assignGraphEdge(data, roads, 'IncidentPoint', 'DestinationID', 'Destination2RoadDist')\n",
    "\n",
    "# # find nearest rescue station\n",
    "# data['NearestRescue'] = nearestRescue4Incidents(data, rescue)\n",
    "# data = data.merge(rescue.loc[:, [\"OBJECTID_nearestRoad\", \"Number\"]], how = 'left', left_on = \"NearestRescue\", right_on = 'OBJECTID_nearestRoad')\n",
    "# data = data.drop(columns = 'OBJECTID_nearestRoad').rename(columns={\"Number\": \"NearestRescueNumber\"})\n",
    "\n",
    "# find the top nearest rescus stations\n",
    "distanceDataFrame = generateDistDf(rescue, graph)\n",
    "obediance = data.apply(obedianceOfShortestPrinciple, distanceDataFrame = distanceDataFrame, axis = 1, result_type = 'expand')\n",
    "data['NearestOrder'] = obediance[0]\n",
    "data['DisobediancePathIncrease'] = obediance[1]\n",
    "\n",
    "# calculate shortest path length and ave speed\n",
    "data['AssumedRouteLength'] = data.apply(shortestRouteLength, distanceDataFrame = distanceDataFrame, axis = 1)\n",
    "data['AverageSpeed'] = data['AssumedRouteLength'] / data['TravelTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "251a2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build adjencency matrix for rescue stations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc86c11",
   "metadata": {},
   "source": [
    "## Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "670aab39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic\n",
    "display(data.loc[:, ['DispatchTime', 'EnRouteTime', 'TravelTime', 'ResponseTime']].mean())\n",
    "display(data['RescueSquadNumber'].value_counts())\n",
    "display(data['CallPriority'].value_counts())\n",
    "display(data['DayOfWeek'].value_counts())\n",
    "display(data['HourInDay'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "773de66d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def incidentMap(data, timeSelectStart, timeSelectEnd, sizeMax, ifLog):\n",
    "    # general spatial dist of incidents\n",
    "    if ifLog == True:\n",
    "        data['LogResponseTime'] = np.log2(data['ResponseTime'])\n",
    "        colorData = \"LogResponseTime\"\n",
    "        range_color = [0, 20]\n",
    "    elif ifLog == False:\n",
    "        colorData = \"ResponseTime\"\n",
    "        range_color = [0, 45000]\n",
    "    px.set_mapbox_access_token(open(\"mapboxToken.txt\").read())\n",
    "    dataSelected = data.loc[timeSelectStart: timeSelectEnd, :].dropna()\n",
    "    fig = px.scatter_mapbox(lat = dataSelected.IncidentPoint.y, lon = dataSelected.IncidentPoint.x, color = dataSelected.ResponseTime,\n",
    "                            color_continuous_scale = px.colors.sequential.Sunsetdark, range_color = range_color, \n",
    "                            size = dataSelected.ResponseTime,\n",
    "                            size_max = sizeMax, \n",
    "                            zoom = 9.5, width = 750, height = 500)\n",
    "    return fig\n",
    "\n",
    "def responsTimeScatter(data):\n",
    "    # show the surge of the response time\n",
    "    fig = go.Figure(data = go.Scatter(x = data.index, y = data['ResponseTime'], mode='markers', marker_color = data['ResponseTime'],)) \n",
    "    fig.update_layout(xaxis_title = \"Datatime\", yaxis_title = \"Response time (s)\",)\n",
    "    return fig\n",
    "\n",
    "def processingTimeProportionDist(data):\n",
    "    df = pd.DataFrame((data.EnRouteTime / data.ResponseTime))\n",
    "    df = df.rename(columns = {0: \"Proportion of Preparation Time\"})\n",
    "    fig = px.histogram(df, x = \"Proportion of Preparation Time\", \n",
    "                        nbins = 75, template = 'seaborn', histnorm = 'probability', opacity = 0.75,\n",
    "                        width = 700, height = 500)\n",
    "    fig.update_layout(yaxis_title = 'Probability')\n",
    "    return fig\n",
    "\n",
    "def proximityOrderDist(data):\n",
    "    df = data.copy()\n",
    "    df['Flooding'] = 'Normal'\n",
    "    df.loc['2016-10-08 11:59:59': '2016-10-09 23:59:59', ['Flooding']] = 'Flooding'\n",
    "    fig = px.histogram(df[df.NearestOrder < 10], \n",
    "                 x = \"NearestOrder\", \n",
    "    #              color = \"Flooding\", \n",
    "                 template = 'seaborn', \n",
    "                 histnorm = 'probability', \n",
    "                 barmode = \"overlay\",\n",
    "                 opacity = 0.75, \n",
    "                 width = 700, height = 500,\n",
    "                )\n",
    "    fig.update_layout(yaxis_title = 'Probability', xaxis_title = 'Proximity Order of Origins')\n",
    "    return fig\n",
    "\n",
    "def distanceIncreaseRatioDist(data):\n",
    "    fig = px.histogram(data[(data.DisobediancePathIncrease > 1)], \n",
    "                 x = \"DisobediancePathIncrease\", \n",
    "                 barmode = \"overlay\",\n",
    "                 template = 'seaborn', histnorm = 'probability', \n",
    "                 opacity = 0.75, \n",
    "                 width = 700, height = 500,)\n",
    "    fig.update_traces(xbins = dict(start = 1, end = 2, size = 0.1))\n",
    "    fig.update_layout(yaxis_title = 'Probability', xaxis_title = 'Travel Distance Increase Percentage')\n",
    "    return fig\n",
    "\n",
    "def responseTimeWithCallPriorityDist(data):\n",
    "    fig = px.histogram(data[data.ResponseTime < 5000], \n",
    "                 x = \"ResponseTime\", \n",
    "                 color = \"CallPriority\", \n",
    "                 barmode = \"overlay\",\n",
    "                 template = 'seaborn', \n",
    "                 histnorm = 'probability', \n",
    "                 opacity = 0.75, \n",
    "                 width = 700, height = 500,\n",
    "                 nbins = 200, \n",
    "                )\n",
    "    fig.update_layout(yaxis_title = 'Probability', xaxis_title = 'Response Time')\n",
    "    return fig\n",
    "\n",
    "def averageSpeedPercentStd(data):\n",
    "    # np.histogram(data.groupby(['OriginRoadID', 'DestinationID']).count().AverageSpeed.values, 10, range = (5, 100)) # keep about 25% of OD when set freqencey above 5\n",
    "    dataSelect = data.loc[:, ['OriginRoadID', 'DestinationID', 'AverageSpeed']]\n",
    "    groupByODCount = dataSelect.groupby(['OriginRoadID', 'DestinationID']).count() # any columns indicates count\n",
    "    groupByODSpeed = dataSelect.groupby(['OriginRoadID', 'DestinationID']).mean()[groupByODCount.AverageSpeed >= 5].rename(columns = {'AverageSpeed': 'AverageSpeed_mean'})\n",
    "    groupByODSpeed['AverageSpeed_std'] = dataSelect.groupby(['OriginRoadID', 'DestinationID']).std()[groupByODCount.AverageSpeed >= 5]\n",
    "    groupByODSpeed['AverageSpeed_stdPercent'] = groupByODSpeed['AverageSpeed_std'] / groupByODSpeed['AverageSpeed_mean']\n",
    "    df = groupByODSpeed['AverageSpeed_stdPercent'].reset_index()\n",
    "    OriginNum = df.OriginRoadID\n",
    "    for origin, num in zip(pd.unique(df.OriginRoadID), range(1, pd.unique(df.OriginRoadID).shape[0] + 1)):\n",
    "        OriginNum = OriginNum.replace(origin, num)\n",
    "    df['OriginNum'] = OriginNum\n",
    "    fig = px.box(df, x = 'OriginNum', y = \"AverageSpeed_stdPercent\", template = 'seaborn', width = 500, height = 750, range_y = (0, 6.5), points = 'suspectedoutliers')\n",
    "    fig.update_layout(yaxis_title = 'Average Speed Percentage Standard Deviation', xaxis_title = 'Rescue Squad Number')\n",
    "    return fig\n",
    "\n",
    "def showWaterOnRoads(roads, figsize = (100, 50), vmax = 6):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    roadsLineWater = roads.loc[:, ['line', 'waterDepth']].set_geometry('line')\n",
    "    ax = roadsLineWater.plot(ax = ax, \n",
    "                        column = 'waterDepth', \n",
    "                        zorder = 5, \n",
    "                        cmap = 'OrRd',\n",
    "                        legend = True,\n",
    "                        vmax = vmax,\n",
    "                       )\n",
    "    cx.add_basemap(ax, crs = roads.crs, source = cx.providers.CartoDB.Positron)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "def showTravelUpRatioOnRoads(roads, figsize = (100, 50), vmax = 10):\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    roadsLineWater = roads.loc[:, ['line', 'travelTimeIncreaseRatio']].set_geometry('line')\n",
    "    ax = roadsLineWater.plot(ax = ax, \n",
    "                        column = 'travelTimeIncreaseRatio', \n",
    "                        zorder = 5, \n",
    "                        cmap = 'OrRd',\n",
    "                        legend = True,\n",
    "                        vmax = vmax,\n",
    "                        vmin = 1,\n",
    "                       )\n",
    "    cx.add_basemap(ax, crs = roads.crs, source = cx.providers.CartoDB.Positron)\n",
    "    ax.set_axis_off()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7306d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processingTimeProportionDist(data)\n",
    "# proximityOrderDist(data)\n",
    "# distanceIncreaseRatioDist(data)\n",
    "# responseTimeWithCallPriorityDist(data)\n",
    "# averageSpeedPercentStd(data)\n",
    "\n",
    "# incidentMap(data, '2016-10-08', '2016-10-09', 16, False)\n",
    "# responsTimeScatter(data)\n",
    "\n",
    "# showWaterOnRoads(roads, (50, 25), 6)\n",
    "# showTravelUpRatioOnRoads(roads, (20, 12), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ca9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b42bff73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xpan88\\AppData\\Local\\Temp\\ipykernel_7612\\118918920.py:1: FutureWarning:\n",
      "\n",
      "The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataGroupByHour = data.groupby('HourInDay').mean()\n",
    "dataGroupByDayOfWeek = data.groupby('DayOfWeek').mean()\n",
    "\n",
    "fig5 = px.bar(dataGroupByHour.reset_index(), y = 'ResponseTime', x = 'HourInDay', text_auto='.3s',)\n",
    "fig5.show()\n",
    "\n",
    "dataGroupByDayOfWeek['DayOfWeek'] = ['Mon.', 'Tue.', 'Wed.', 'Thu.', 'Fri.', 'Sat.', 'Sun.']\n",
    "fig6 = px.bar(dataGroupByDayOfWeek, y = 'ResponseTime', x = 'DayOf Week', text_auto = '.3s',)\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff630f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGroupByHour['Time period'] = ['0AM - 3AM', '0AM - 3AM', '0AM - 3AM',\n",
    "                                  '3AM - 6AM', '3AM - 6AM', '3AM - 6AM', \n",
    "                                  '6AM - 9AM', '6AM - 9AM', '6AM - 9AM',\n",
    "                                  '9AM - 12PM', '9AM - 12PM', '9AM - 12PM',\n",
    "                                  '12PM - 3PM', '12PM - 3PM', '12PM - 3PM',\n",
    "                                  '3PM - 6PM', '3PM - 6PM', '3PM - 6PM', \n",
    "                                  '6PM - 9PM', '6PM - 9PM','6PM - 9PM',\n",
    "                                  '9PM - 12PM', '9PM - 12PM', '9PM - 12PM',] \n",
    "\n",
    "dataGroupByTimePeriod = dataGroupByHour.groupby('Time period').mean()\n",
    "fig = px.bar(dataGroupByTimePeriod.reset_index(), y = 'Response Time', x = 'Time period', text_auto='.3s',)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6 = go.Figure()\n",
    "hourList  = range(0, 24)\n",
    "for hour in hourList:\n",
    "    fig6.add_trace(go.Box(\n",
    "        y = dataProcessed[:'2016-10-08'].loc[lambda row: row.index.hour == hour]['Response Time'].tolist(),\n",
    "        name = 'Hour ' + str(hour),\n",
    "        jitter=0.3,\n",
    "        pointpos=-1.8,\n",
    "        boxpoints=False, # represent all points\n",
    "        marker_color='rgb(7,40,89)',\n",
    "        line_color='rgb(7,40,89)'\n",
    "    ))\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc827b3a",
   "metadata": {},
   "source": [
    "# Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ccdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation time prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef639b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample data\n",
    "def makeODmatrix(dataOneHourIndex, timeType = 'ResponseTime'):\n",
    "    dataOneHour = data.loc[dataOneHourIndex.index, :]\n",
    "    dataOneHour = dataOneHour.loc[:, ['OriginRoadID', 'DestinationID', timeType]]\n",
    "    dataOneHour = dataOneHour.groupby(by = ['OriginRoadID', 'DestinationID'], dropna = True).mean()\n",
    "    \n",
    "    ODmatrix_df = pd.DataFrame(index = rescue.OBJECTID_nearestRoad.values, columns = roads.OBJECTID.values)\n",
    "    for indexes in dataOneHour.index:\n",
    "        ODmatrix_df.loc[int(indexes[0]), int(indexes[1])] = dataOneHour.loc[indexes].values[0]\n",
    "    return ODmatrix_df.to_numpy()\n",
    "\n",
    "dataByHour = data.resample(pd.Timedelta(1, \"hour\"), closed = 'left', label = 'left', origin = 'end_day').apply(makeODmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0f96a74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallDateTime\n",
       "2016-01-01 00:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-01-01 01:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-01-01 02:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-01-01 03:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-01-01 04:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "                                             ...                        \n",
       "2016-10-15 19:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-10-15 20:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-10-15 21:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-10-15 22:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "2016-10-15 23:00:00    [[nan, nan, nan, nan, nan, nan, nan, nan, nan,...\n",
       "Freq: H, Length: 6936, dtype: object"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataByHour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9643c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebfe351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "11.7\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487292f",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61ebe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "fig, ax = plt.subplots(figsize=(30, 7.5))\n",
    "t = range(len(dataTest_y_Flood))\n",
    "ax.plot(t, predictions, label = 'prediction')\n",
    "ax.plot(t, dataTest_y_Flood, label = 'ground truth')\n",
    "ax.set_xlabel('Incident', fontsize = 20)\n",
    "ax.set_ylabel('If accessible', fontsize = 20)\n",
    "ax.legend(fontsize = 20)\n",
    "ax.tick_params(axis='both', labelsize = 15)\n",
    "#ax.set_title('Prediction vs Ground truth', fontsize = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "print(classification_report(dataTest_y_Flood, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57790fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculateWaste (row):\n",
    "    if row['If accessible Real'] == 1 and row['If accessible Predicted'] == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculateUnknownDanger (row):\n",
    "    if row['If accessible Real'] == 0 and row['If accessible Predicted'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "    \n",
    "dataFlood = dataProcessed.loc['2016-10-09' : '2016-10-09']\n",
    "dataFlood['If accessible Real'] = dataFlood['Accessibility'].astype('int64')\n",
    "dataFlood['If accessible Predicted'] = [element[0] for element in predictions]\n",
    "dataFlood['Waste'] = dataFlood.apply(calculateWaste, axis = 1)\n",
    "dataFlood['Unknown Danger'] = dataFlood.apply(calculateUnknownDanger, axis = 1)\n",
    "dataFlood['Error Type'] = (dataFlood['Waste'] + dataFlood['Unknown Danger'] * 2).astype('string') # 1 mean wastes, 2 means potential danger\n",
    "dataFlood['Error Type'] = dataFlood['Error Type'].replace('1', 'Type 1').replace('2', 'Type 2')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "display(dataFlood)\n",
    "\n",
    "# visualization\n",
    "px.set_mapbox_access_token(open(\"mapboxToken.txt\").read())\n",
    "fig1 = px.scatter_mapbox(dataFlood.loc[lambda df: df['Error Type'] != '0'], \n",
    "                        lat=\"latitude\", lon=\"longitude\",  \n",
    "                        color = \"Error Type\", #size = \"Response Time\",\n",
    "                        size_max = 15, zoom = 10, width = 575, height = 500)\n",
    "fig1.show()\n",
    "\n",
    "fig2 = px.scatter_mapbox(dataFlood.loc[lambda df: (df['Error Type'] != '0') &\n",
    "                                      (df['Error Type'] != '1')], \n",
    "                        lat=\"latitude\", lon=\"longitude\",  \n",
    "                        color = \"Error Type\", size = \"Response Time\",\n",
    "                        size_max = 30, zoom = 10, width = 550, height = 500)\n",
    "fig2.update_layout(showlegend=False)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "shp = gpd.read_file('./data/VB_City_Boundary.geojson')\n",
    "shp.crs = 'CRS84'\n",
    "\n",
    "# generate all points\n",
    "numOfPointsOneDimX = 50\n",
    "deltaX = shp.bounds.maxx - shp.bounds.minx\n",
    "deltaY = shp.bounds.maxy - shp.bounds.miny\n",
    "numOfPointsOneDimY = numOfPointsOneDimX * (deltaY / deltaX)\n",
    "\n",
    "xCorList = np.arange(float(shp.bounds.minx), float(shp.bounds.maxx), float(deltaX / numOfPointsOneDimX))\n",
    "yCorList = np.arange(float(shp.bounds.miny), float(shp.bounds.maxy), float(deltaY / numOfPointsOneDimY))\n",
    "xyPointList = [Point(x, y) for x in xCorList for y in yCorList]\n",
    "\n",
    "# select points within the city\n",
    "samplePoints = gpd.GeoSeries(xyPointList)\n",
    "samplePoints.crs = 'CRS84'\n",
    "withinOrNot = samplePoints.within(shp['geometry'].values[0])\n",
    "gdf = pd.concat([samplePoints, withinOrNot], axis = 1)\n",
    "gdf.crs = 'CRS84'\n",
    "gdfSelected = gdf.loc[gdf[1] == True]\n",
    "display(gdfSelected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the prediction for these points\n",
    "gdfSelected['latitude'] = gdfSelected[0].values.y\n",
    "gdfSelected['longitude'] = gdfSelected[0].values.x\n",
    "\n",
    "def addTimeFeature(gdf, hourInDay, dayOfWeek):\n",
    "    gdf_out = gdf.copy()\n",
    "    gdf_out['Hour in Day'] = hourInDay\n",
    "    gdf_out['Day of Week'] = dayOfWeek\n",
    "    return gdf_out\n",
    "\n",
    "gdfSelected_withTime = addTimeFeature(gdfSelected, 0, 6) #The day of week is 1, because it is the normalized value, flooding day is Sunday\n",
    "for hour in range(23):\n",
    "    gdfSelected_withTime = pd.concat([gdfSelected_withTime, addTimeFeature(gdfSelected, hour + 1, 6)])\n",
    "\n",
    "gdfForPrediction = gdfSelected_withTime.reset_index().loc[:,['latitude', 'longitude', 'Hour in Day']]\n",
    "gdfForPrediction_norm = normalizer(gdfForPrediction.copy())\n",
    "gdfForPrediction_norm['Day of Week'] = 1 #The day of week is 1, because it is the normalized value, flooding day is Sunday\n",
    "gdfForPrediction_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsFull = model.predict(gdfForPrediction_norm.values)\n",
    "predictionsFull = np.where(predictionsFull < 0.5, 0, 1).tolist()\n",
    "print(len(predictionsFull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFloodFull = gdfForPrediction.copy()\n",
    "dataFloodFull['If accessible'] = [element[0] for element in predictionsFull]\n",
    "dataFloodFull['If accessible'] = dataFloodFull['If accessible'].astype('string').replace('1', 'Accessible').replace('0', 'Inaccessible')\n",
    "display(dataFloodFull)\n",
    "\n",
    "# visualization\n",
    "px.set_mapbox_access_token(open(\"mapboxToken.txt\").read())\n",
    "fig = px.scatter_mapbox(dataFloodFull.loc[dataFloodFull['Hour in Day'] == 23], \n",
    "                        lat = \"latitude\", lon = \"longitude\",  \n",
    "                        color = \"If accessible\", #size = \"Response Time\",\n",
    "                        zoom = 9.5, opacity = 0.7, width = 600, height =700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "90a8183b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:emsTravelTime]",
   "language": "python",
   "name": "conda-env-emsTravelTime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
