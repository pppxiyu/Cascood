{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mA2lYErBQmw2",
   "metadata": {
    "id": "mA2lYErBQmw2"
   },
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VgY-2WryPIWf",
   "metadata": {
    "id": "VgY-2WryPIWf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687895448482,
     "user_tz": 420,
     "elapsed": 3286,
     "user": {
      "displayName": "Xiyu Pan",
      "userId": "05627784278786645916"
     }
    },
    "outputId": "93695803-7bfa-4eb8-cb40-007df842ff08"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',  force_remount=True)\n",
    "%cd '/content/drive/Othercomputers/Lab Desktop 2/Desktop/Research_EMS'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9WGTF8dP4ZF",
   "metadata": {
    "id": "f9WGTF8dP4ZF"
   },
   "source": [
    "%pip install geopandas\n",
    "%pip install rasterio\n",
    "%pip install momepy\n",
    "%pip install contextily\n",
    "%pip install ray\n",
    "%pip install darts\n",
    "%pip install folium\n",
    "%pip install mapclassify\n",
    "%pip install xyzservices"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a0858",
   "metadata": {
    "id": "f82a0858",
    "scrolled": true
   },
   "source": [
    "from preprocess import *\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import xyzservices.providers as xyz\n",
    "from collections import Counter\n",
    "import plotly.figure_factory as ff"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7c7835cd",
   "metadata": {
    "id": "7c7835cd"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJ6D71EgYm1o",
   "metadata": {
    "id": "UJ6D71EgYm1o"
   },
   "source": [
    "## Preprocess large data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3dbe8",
   "metadata": {
    "id": "3cc3dbe8"
   },
   "source": [
    "# preprocess and save ambulance data\n",
    "# data = rawData('./data/ambulance/virginiaBeach_ambulance_timeData.csv')\n",
    "# data = addOrigin(data, './data/rescueTeamLocation/rescueStations.txt')\n",
    "# data = geoCoding(data.loc['2013-01-01' : '2013-12-31', :], './data/ambulance/geocoded_saved/20130101-20131231.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be131d1e",
   "metadata": {
    "id": "be131d1e"
   },
   "source": [
    "# roads = readRoads('./data/roads/Streets.shp')\n",
    "# roads = makeSurface4Lines(roads, './data/roads/Road_Surfaces.shp', scale = 2.7)\n",
    "# roads = getWaterDepthOnRoads(roads, './data/inundation/tifData/depth_objID_35.tif', './data/inundation/croppedByRoads/croppedByRoads.tif')\n",
    "\n",
    "# roads.drop(['line', 'midpoint','buffers','buffersUnscaled'], axis = 1).to_file('./data/roads/savedInundatedRoads/roads_with_objID_35.shp')\n",
    "# roads['line'].to_file('./data/roads/savedInundatedRoads/roads_with_objID_35_line.shp')\n",
    "# roads['midpoint'].to_file('./data/roads/savedInundatedRoads/roads_with_objID_35_midpoint.shp')\n",
    "# roads['buffers'].to_file('./data/roads/savedInundatedRoads/roads_with_objID_35_buffers.shp')\n",
    "# roads['buffersUnscaled'].to_file('./data/roads/savedInundatedRoads/roads_with_objID_35_buffersUnscaled.shp')\n",
    "\n",
    "# consider bridges in road network (PENDING)\n",
    "# bridges = gpd.read_file('./data/roads/bridges/bridgePolygon.shp').to_crs(str(inundation.crs))\n",
    "# inundation_cropped = inundationCutter(inundation, bridges, True, True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "Pv3b-YEgYtgy",
   "metadata": {
    "id": "Pv3b-YEgYtgy"
   },
   "source": [
    "## Reload and additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038ea46",
   "metadata": {
    "id": "a038ea46"
   },
   "source": [
    "# reload data and build geoDataFrame\n",
    "data = reLoadData('./data/ambulance/geocoded_saved/20160101-20161015.csv')\n",
    "roads = reLoadRoads('./data/roads/savedInundatedRoads/roads_with_objID_35.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1c019",
   "metadata": {
    "id": "4cd1c019"
   },
   "source": [
    "# create graph\n",
    "graph = roads2Graph(roads)\n",
    "# showGraphRoads(roads, graph)\n",
    "# NOTE: the graph is un-directed right now, the logic should be checked if changed to directed\n",
    "\n",
    "# read the location of rescue squads and attach them to nodes\n",
    "rescue = readRescue('./data/rescueTeamLocation/rescueStations.txt', 'EPSG:4326', roads)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91844a4f",
   "metadata": {
    "id": "91844a4f"
   },
   "source": [
    "# additional info for descriptive analysis\n",
    "# assign records to graph edges\n",
    "data = assignGraphEdge(data, roads, 'RescueSquadPoint', 'OriginRoadID', 'Origin2RoadDist')\n",
    "data = assignGraphEdge(data, roads, 'IncidentPoint', 'DestinationID', 'Destination2RoadDist')\n",
    "\n",
    "# find nearest rescue station\n",
    "# data = nearestRescueStation(data, rescue)\n",
    "\n",
    "# find the top nearest rescue stations\n",
    "data = nearnessObediance(data, rescue, graph)\n",
    "\n",
    "# calculate shortest path length and ave speed\n",
    "data = assumedAveSpeed(data, rescue, graph)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e78e5395",
   "metadata": {
    "id": "e78e5395"
   },
   "source": [
    "# Disruption analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f875e4",
   "metadata": {
    "id": "97f875e4",
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687896567967,
     "user_tz": 420,
     "elapsed": 840619,
     "user": {
      "displayName": "Xiyu Pan",
      "userId": "05627784278786645916"
     }
    },
    "outputId": "f2a77fe8-3e6e-4735-c317-45c563cb00be"
   },
   "source": [
    "def _addPathLen2Graph(graph, rescue, weight, newAttribute_rescueSquad, newAttribute_pathLen, newAttribute_pathList = None):\n",
    "    # some roads are disconnected from all the rescue station even in normal time (as the raw data indicates)\n",
    "    voronoi = nx.voronoi_cells(graph, set(rescue.OBJECTID_nearestRoad.unique()), weight = weight)\n",
    "    for rescueSquad, destinations in zip(voronoi.keys(), voronoi.values()):\n",
    "        if rescueSquad == 'unreachable':\n",
    "            print(len(destinations), 'nodes are unreachable when building voronoi for', newAttribute_pathLen)\n",
    "            for des in destinations:\n",
    "                graph.nodes[des][newAttribute_rescueSquad] = np.nan\n",
    "                graph.nodes[des][newAttribute_pathLen] = math.inf # set path len to inf if it's disconnected from rescues\n",
    "#                 print('NOTE: node', des, 'is unreachable when building voronoi for', newAttribute_pathLen)\n",
    "        else:\n",
    "            for des in destinations:\n",
    "                shortestPathLen = nx.shortest_path_length(graph, source = rescueSquad, target = des, weight = weight)\n",
    "                graph.nodes[des][newAttribute_pathLen] = shortestPathLen\n",
    "                graph.nodes[des][newAttribute_rescueSquad] = rescueSquad\n",
    "                if newAttribute_pathList:\n",
    "                    shortestPath = nx.shortest_path(graph, source = rescueSquad, target = des, weight = weight)\n",
    "                    graph.nodes[des][newAttribute_pathList] = shortestPath\n",
    "                if shortestPathLen == 0:\n",
    "                    graph.nodes[des][newAttribute_pathLen] = 1\n",
    "                if shortestPathLen == math.inf:\n",
    "                    graph.nodes[des][newAttribute_rescueSquad] = math.inf\n",
    "    return graph, voronoi\n",
    "\n",
    "def _addDisruption(graph, roads, newAttribute = 'weightWithDisruption', threshold = 3):\n",
    "    nx.set_edge_attributes(graph, nx.get_edge_attributes(graph, \"weight\"), newAttribute)\n",
    "    disruptedRoads = roads[roads['waterDepth'] >= threshold]['OBJECTID'].to_list()\n",
    "    for disruption in disruptedRoads:\n",
    "        for edge in graph.edges(disruption):\n",
    "            graph.edges()[edge][newAttribute] = math.inf # set edge weight to inf if it's disrupted by inundation\n",
    "    return graph\n",
    "\n",
    "def _changeValue4DisruptedRoad(roads, graph, threshold = 3):\n",
    "    # the disrupted road itself is not disconnected, so assign the shortestPath of adjancent road to this road\n",
    "    for disruption in roads[roads['waterDepth'] >= threshold]['OBJECTID'].to_list():\n",
    "        pathLen = []\n",
    "        edgeNum = []\n",
    "        for edge in graph.edges(disruption):\n",
    "            pathLen.append(graph.nodes()[edge[1]]['shortestPathLenWithDisruption'])\n",
    "            edgeNum.append(edge[1])\n",
    "        if pathLen != []: # in case there are disconnected single node\n",
    "            graph.nodes()[disruption]['shortestPathLenWithDisruption'] = min(pathLen)\n",
    "            if min(pathLen) != math.inf:\n",
    "                graph.nodes()[disruption]['rescueAssignedWithDisruption'] = edgeNum[pathLen.index(min(pathLen))]\n",
    "            else:\n",
    "                graph.nodes()[disruption]['rescueAssignedWithDisruption'] = np.nan\n",
    "    return graph\n",
    "\n",
    "def runRoutingWithDisruption(graph, rescue, roads):\n",
    "    graph, _ = _addPathLen2Graph(graph, rescue, 'weight', 'rescueAssigned', 'shortestPathLen', 'shortestPathList')\n",
    "    graphDisrupted = _addDisruption(graph, roads, threshold = 1)\n",
    "    graph, _ = _addPathLen2Graph(graphDisrupted, rescue, 'weightWithDisruption', 'rescueAssignedWithDisruption', 'shortestPathLenWithDisruption', 'shortestPathListWithDisruption')\n",
    "    graph = _changeValue4DisruptedRoad(roads, graph, threshold = 1)\n",
    "    return graph\n",
    "\n",
    "def getDisruptionRatio(graph):\n",
    "    nx.set_node_attributes(graph,\n",
    "                           {x[0]: y[1]/x[1] if y[1]/x[1] != math.inf else np.nan \\\n",
    "                            for x, y in zip(nx.get_node_attributes(graph, \"shortestPathLen\").items(),\n",
    "                                            nx.get_node_attributes(graph, \"shortestPathLenWithDisruption\").items() ) },\n",
    "                           'travelTimeIncreaseRatio')\n",
    "    roads['travelTimeIncreaseRatio'] = roads['OBJECTID'].map(nx.get_node_attributes(graph, \"travelTimeIncreaseRatio\"))\n",
    "    return graph\n",
    "\n",
    "def removeDisconnectedNodes(graph):\n",
    "    largest_cc = max(nx.connected_components(graph), key = len)\n",
    "    nodes_to_remove = [node for node in graph.nodes if node not in largest_cc]\n",
    "    graph.remove_nodes_from(nodes_to_remove)\n",
    "    return graph\n",
    "\n",
    "# calculate ratios\n",
    "graph = runRoutingWithDisruption(graph, rescue, roads)\n",
    "graph = getDisruptionRatio(graph)\n",
    "graph = removeDisconnectedNodes(graph)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "roads_fromGraph = pd.DataFrame.from_dict(dict(graph.nodes(data = True)), orient = 'index')\n",
    "roads_fromGraph = gpd.GeoDataFrame(roads_fromGraph, geometry = 'line').set_crs('EPSG:3857').to_crs('EPSG:4326')"
   ],
   "metadata": {
    "id": "3GxqLYDY-plO"
   },
   "id": "3GxqLYDY-plO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc86c11",
   "metadata": {
    "id": "7fc86c11"
   },
   "source": [
    "# Figs and Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773de66d",
   "metadata": {
    "id": "773de66d",
    "scrolled": false
   },
   "source": [
    "def showRoadsWithValues(roads, values, vmax = 6, vmin = 0, saveAddr = None, figsize = (10, 10), cmap = 'OrRd'):\n",
    "    roads = roads.set_geometry('line')\n",
    "    roads = roads.to_crs('EPSG:4326')\n",
    "    samplePoints_lat = roads.apply(lambda row: row.line.coords[0][1], axis = 1)\n",
    "    samplePoints_lon = roads.apply(lambda row: row.line.coords[0][0], axis = 1)\n",
    "    roads = roads.loc[samplePoints_lat > 36.72]\n",
    "    roads = roads.loc[samplePoints_lon < -75.98]\n",
    "    roads = roads.loc[samplePoints_lon > -76.16]\n",
    "\n",
    "    # roads = roads.set_geometry('midpoint') # open it if draw points\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "    cax = make_axes_locatable(ax).append_axes(\"right\", size = \"2%\", pad = 0.1,)\n",
    "    cax.tick_params(labelsize = 25)\n",
    "\n",
    "    roads.plot(\n",
    "        ax = ax,\n",
    "        cax = cax,\n",
    "        column = values,\n",
    "        zorder = 5,\n",
    "        cmap = cmap,\n",
    "        # color = 'deeppink',\n",
    "        legend = True,\n",
    "        vmax = vmax,\n",
    "        vmin = vmin,\n",
    "        # linewidth = roads[values], # open if adjust line width\n",
    "        linewidth = 3,\n",
    "        # marker = 'o', # open it if draw points\n",
    "        # markersize = 600, # open it if draw points\n",
    "        )\n",
    "\n",
    "    cx.add_basemap(ax, crs = roads.crs, source = cx.providers.CartoDB.Positron, zoom = 13)\n",
    "    cx.add_basemap(ax, crs = roads.crs, source = cx.providers.Stamen.TonerLabels, zoom = 13, alpha = 0.6)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    if saveAddr:\n",
    "        fig.savefig(saveAddr, dpi = 300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show(ax)\n",
    "\n",
    "\n",
    "def incidentMap(data, timeSelectStart, timeSelectEnd, sizeMax, ifLog):\n",
    "    # general spatial dist of incidents\n",
    "    if ifLog == True:\n",
    "        data['LogResponseTime'] = np.log2(data['ResponseTime'])\n",
    "        colorData = \"LogResponseTime\"\n",
    "        range_color = [0, 20]\n",
    "    elif ifLog == False:\n",
    "        colorData = \"ResponseTime\"\n",
    "        range_color = [0, 45000]\n",
    "    px.set_mapbox_access_token(open(\"mapboxToken.txt\").read())\n",
    "    dataSelected = data.loc[timeSelectStart: timeSelectEnd, :].dropna()\n",
    "    fig = px.scatter_mapbox(lat = dataSelected.IncidentPoint.y, lon = dataSelected.IncidentPoint.x, color = dataSelected.ResponseTime,\n",
    "                            color_continuous_scale = px.colors.sequential.Sunsetdark, range_color = range_color,\n",
    "                            size = dataSelected.ResponseTime,\n",
    "                            size_max = sizeMax,\n",
    "                            zoom = 9.5, width = 750, height = 500)\n",
    "    return fig\n",
    "\n",
    "def responsTimeScatter(data):\n",
    "    # show the surge of the response time\n",
    "    fig = go.Figure(data = go.Scatter(x = data.index, y = data['ResponseTime'], mode='markers', marker_color = data['ResponseTime'],))\n",
    "    fig.update_layout(xaxis_title = \"Datatime\", yaxis_title = \"Response time (s)\",)\n",
    "    return fig\n",
    "\n",
    "def processingTimeProportionDist(data):\n",
    "    df = pd.DataFrame((data.EnRouteTime / data.ResponseTime))\n",
    "    df = df.rename(columns = {0: \"Proportion of Preparation Time\"})\n",
    "    fig = px.histogram(df, x = \"Proportion of Preparation Time\",\n",
    "                        nbins = 75, template = 'seaborn', histnorm = 'probability', opacity = 0.75,\n",
    "                        width = 700, height = 500)\n",
    "    fig.update_layout(yaxis_title = 'Probability')\n",
    "    return fig\n",
    "\n",
    "def proximityOrderDist(data):\n",
    "    df = data.copy()\n",
    "    df['Flooding'] = 'Normal'\n",
    "    df.loc['2016-10-08 11:59:59': '2016-10-09 23:59:59', ['Flooding']] = 'Flooding'\n",
    "    fig = px.histogram(df[df.NearestOrder < 10],\n",
    "                 x = \"NearestOrder\",\n",
    "    #              color = \"Flooding\",\n",
    "                 template = 'seaborn',\n",
    "                 histnorm = 'probability',\n",
    "                 barmode = \"overlay\",\n",
    "                 opacity = 0.75,\n",
    "                 width = 700, height = 500,\n",
    "                )\n",
    "    fig.update_layout(yaxis_title = 'Probability', xaxis_title = 'Proximity Order of Origins')\n",
    "    return fig\n",
    "\n",
    "def distanceIncreaseRatioDist(data):\n",
    "    fig = px.histogram(data[(data.DisobediancePathIncrease > 1)],\n",
    "                 x = \"DisobediancePathIncrease\",\n",
    "                 barmode = \"overlay\",\n",
    "                 template = 'seaborn', histnorm = 'probability',\n",
    "                 opacity = 0.75,\n",
    "                 width = 700, height = 500,)\n",
    "    fig.update_traces(xbins = dict(start = 1, end = 2, size = 0.1))\n",
    "    fig.update_layout(yaxis_title = 'Probability', xaxis_title = 'Travel Distance Increase Percentage')\n",
    "    return fig\n",
    "\n",
    "def responseTimeWithCallPriorityDist(data):\n",
    "    fig = px.histogram(data[data.ResponseTime < 5000],\n",
    "                 x = \"ResponseTime\",\n",
    "                 color = \"CallPriority\",\n",
    "                 barmode = \"overlay\",\n",
    "                 template = 'seaborn',\n",
    "                 histnorm = 'probability',\n",
    "                 opacity = 0.75,\n",
    "                 width = 700, height = 500,\n",
    "                 nbins = 200,\n",
    "                )\n",
    "    fig.update_layout(yaxis_title = 'Probability', xaxis_title = 'Response Time')\n",
    "    return fig\n",
    "\n",
    "def averageSpeedPercentStd(data):\n",
    "    # np.histogram(data.groupby(['OriginRoadID', 'DestinationID']).count().AverageSpeed.values, 10, range = (5, 100)) # keep about 25% of OD when set freqencey above 5\n",
    "    dataSelect = data.loc[:, ['OriginRoadID', 'DestinationID', 'AverageSpeed']]\n",
    "    groupByODCount = dataSelect.groupby(['OriginRoadID', 'DestinationID']).count() # any columns indicates count\n",
    "    groupByODSpeed = dataSelect.groupby(['OriginRoadID', 'DestinationID']).mean()[groupByODCount.AverageSpeed >= 5].rename(columns = {'AverageSpeed': 'AverageSpeed_mean'})\n",
    "    groupByODSpeed['AverageSpeed_std'] = dataSelect.groupby(['OriginRoadID', 'DestinationID']).std()[groupByODCount.AverageSpeed >= 5]\n",
    "    groupByODSpeed['AverageSpeed_stdPercent'] = groupByODSpeed['AverageSpeed_std'] / groupByODSpeed['AverageSpeed_mean']\n",
    "    df = groupByODSpeed['AverageSpeed_stdPercent'].reset_index()\n",
    "    OriginNum = df.OriginRoadID\n",
    "    for origin, num in zip(pd.unique(df.OriginRoadID), range(1, pd.unique(df.OriginRoadID).shape[0] + 1)):\n",
    "        OriginNum = OriginNum.replace(origin, num)\n",
    "    df['OriginNum'] = OriginNum\n",
    "    fig = px.box(df, x = 'OriginNum', y = \"AverageSpeed_stdPercent\", template = 'seaborn', width = 500, height = 750, range_y = (0, 6.5), points = 'suspectedoutliers')\n",
    "    fig.update_layout(yaxis_title = 'Average Speed Percentage Standard Deviation', xaxis_title = 'Rescue Squad Number')\n",
    "    return fig\n",
    "\n",
    "def showAveTime(time, by):\n",
    "    dataGroupByHour = data.loc[:, [time, by]].groupby(by).mean()\n",
    "    fig = px.bar(dataGroupByHour.reset_index(), y = time, x = by, text_auto='.3s',)\n",
    "    return fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306d5ad",
   "metadata": {
    "id": "7306d5ad"
   },
   "source": [
    "# # incident time features\n",
    "# responsTimeScatter(data)\n",
    "# responseTimeWithCallPriorityDist(data)\n",
    "# processingTimeProportionDist(data)\n",
    "# showAveTime('ResponseTime', 'HourInDay')\n",
    "# showAveTime('ResponseTime', 'DayOfWeek')\n",
    "\n",
    "# # incident spatial features\n",
    "# incidentMap(data, '2016-10-08', '2016-10-09', 16, False)\n",
    "# proximityOrderDist(data)\n",
    "# distanceIncreaseRatioDist(data)\n",
    "\n",
    "# incident speed features\n",
    "averageSpeedPercentStd(data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# distance to nearest rescue station in normal time\n",
    "roads_fromGraph_cor = roads_fromGraph.apply(lambda row: row.line.coords[0][1], axis = 1)\n",
    "roads_fromGraph_north = roads_fromGraph['shortestPathLen'].loc[roads_fromGraph_cor > 36.72]\n",
    "showRoadsWithValues(roads_fromGraph, 'shortestPathLen',\n",
    "                    vmax = roads_fromGraph_north.max(),\n",
    "                    vmin = 0,\n",
    "                    saveAddr = './fig/normal_toRescue',\n",
    "                    figsize = (30,30))"
   ],
   "metadata": {
    "id": "3GkK6o4Ijr6a"
   },
   "id": "3GkK6o4Ijr6a",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# average response time in normal time map\n",
    "data_0 = data[data.EntryDateTime < '2016-10-01 00:00:00'][['ResponseTime', 'DestinationID']].groupby('DestinationID').mean()\n",
    "roads_0 = roads.merge(data_0, left_on = 'OBJECTID', right_index = True, how = 'left')\n",
    "roads_0 = roads_0[~roads_0.ResponseTime.isna()]\n",
    "roads_0 = roads_0.sort_values(by = ['ResponseTime'])\n",
    "roads_0 = roads_0.iloc[: math.floor(len(roads_0) * 0.90)]\n",
    "roads_0 = roads_0[roads_0.ResponseTime != 0]\n",
    "roads_0\n",
    "showRoadsWithValues(roads_0, 'ResponseTime',\n",
    "                    vmax = 759,\n",
    "                    vmin = 99,\n",
    "                    saveAddr = './fig/normal_responseTimeMap',\n",
    "                    figsize = (30,30))"
   ],
   "metadata": {
    "id": "yRBiE46Ovw4C"
   },
   "id": "yRBiE46Ovw4C",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# inundation depth map\n",
    "showRoadsWithValues(roads, 'waterDepth', vmax = 6, vmin = 0, saveAddr = './fig/inundation', figsize = (30,30), cmap = 'Blues')"
   ],
   "metadata": {
    "id": "82vrp7N-TZ4U"
   },
   "id": "82vrp7N-TZ4U",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show road being cut off\n",
    "showRoadsWithValues(roads[roads.waterDepth > 1], 'waterDepth', vmax = 13, vmin = 1, saveAddr = './fig/inundation_cutOff', figsize = (30,30))"
   ],
   "metadata": {
    "id": "vSyHeyAnwP0j"
   },
   "id": "vSyHeyAnwP0j",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get all route being cut off\n",
    "roadsCutOff = roads[roads.waterDepth > 1].OBJECTID.values\n",
    "routesCutOff = []\n",
    "for nodeName in list(graph.nodes()):\n",
    "    route = graph.nodes[nodeName]['shortestPathList']\n",
    "    if any(cutOff in route for cutOff in roadsCutOff):\n",
    "        routesCutOff.append(route)\n",
    "# count occurance for nodes\n",
    "nodesCount = dict(Counter([node for route in routesCutOff for node in route]))\n",
    "# add them to df\n",
    "roads_0 = roads.copy()\n",
    "roads_0['cutOffCount'] = np.log2(roads_0['OBJECTID'].map(nodesCount))\n",
    "# draw map using the new df\n",
    "showRoadsWithValues(roads_0, 'cutOffCount', vmax = roads_0.cutOffCount.max(), vmin = 0, saveAddr = './fig/routes_cutOff', figsize = (30,30), cmap = 'Oranges')"
   ],
   "metadata": {
    "id": "wvqYtzVjT844"
   },
   "id": "wvqYtzVjT844",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get the affected destination of the EMS routes\n",
    "roads_0 = roads_0.set_geometry('midpoint')\n",
    "desCount = dict(Counter([route[-1] for route in routesCutOff]))\n",
    "roads_0['ifAffected'] = roads_0['OBJECTID'].map(desCount)\n",
    "roads_0_select = roads_0[roads_0['ifAffected'] == 1]\n",
    "showRoadsWithValues(roads_0_select, 'ifAffected', vmax = 1, vmin = 0, saveAddr = './fig/routes_destinations', figsize = (30,30), cmap = 'Oranges')"
   ],
   "metadata": {
    "id": "S7j0RUmoFOSB"
   },
   "id": "S7j0RUmoFOSB",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# micro scale impact and route change\n",
    "# roads[(roads.travelTimeIncreaseRatio > 2) & (roads.travelTimeIncreaseRatio < 4)] # select des using the roads df\n",
    "originalRoute = graph.nodes[166]['shortestPathList']\n",
    "updatedRoute = graph.nodes[166]['shortestPathListWithDisruption']\n",
    "showRoadsWithValues(roads[roads.OBJECTID.isin(originalRoute)], '', saveAddr = './fig/routes_micro_original', figsize = (30,30))\n",
    "showRoadsWithValues(roads[roads.OBJECTID.isin(updatedRoute)], '', saveAddr = './fig/routes_micro_updated', figsize = (30,30))"
   ],
   "metadata": {
    "id": "hw-vAiSNjpEH"
   },
   "id": "hw-vAiSNjpEH",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# travel distance increase ratio estimation\n",
    "roads_0 = roads.copy()\n",
    "roads_0['travelTimeIncreaseRatio'] = np.log10(roads_0['travelTimeIncreaseRatio'])\n",
    "showRoadsWithValues(roads_0, 'travelTimeIncreaseRatio',\n",
    "                    vmax = 1,\n",
    "                    vmin = 0,\n",
    "                    saveAddr = './fig/response_increaseRatio',\n",
    "                    figsize = (30,30))"
   ],
   "metadata": {
    "id": "3rTGiYkcRkP0"
   },
   "id": "3rTGiYkcRkP0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show the real increase ratio\n",
    "ratios = roads_fromGraph.travelTimeIncreaseRatio.values\n",
    "ratios = ratios[~np.isnan(ratios)]\n",
    "fig = ff.create_distplot([ratios], ['Increase Ratio'], show_hist = False, show_curve = False)\n",
    "fig.update_layout(font_size = 30)"
   ],
   "metadata": {
    "id": "rYofWyVpCY9_"
   },
   "id": "rYofWyVpCY9_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show the distance dist before and after inundation\n",
    "fig = ff.create_distplot([roads_fromGraph.shortestPathLen,\n",
    "                          roads_fromGraph.shortestPathLenWithDisruption[roads_fromGraph.shortestPathLenWithDisruption != np.inf]\n",
    "                          ], ['Before Inundation', 'After Inundation'],\n",
    "                         show_rug = False,\n",
    "                         bin_size = 250,\n",
    "                         )\n",
    "fig.update_layout(\n",
    "    width = 1000,\n",
    "    height = 500,\n",
    "    font_size = 15\n",
    "    )\n",
    "fig.show()"
   ],
   "metadata": {
    "id": "fZeu_nE2GoI0"
   },
   "id": "fZeu_nE2GoI0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# draw the real response time inrease ratio map\n",
    "\n",
    "# fill the nan in dataDuringFlooding\n",
    "dataDuringFlooding = data.loc[(data.index > '2016-10-08 18:00:00') & (data.index < '2016-10-9 18:00:00')]\n",
    "dataDuringFlooding_na = dataDuringFlooding[dataDuringFlooding.ResponseTime.isna()]\n",
    "dataDuringFlooding_na = dataDuringFlooding_na.reset_index()\n",
    "dataDuringFlooding_na.CloseDateTime = pd.to_datetime(dataDuringFlooding_na.CloseDateTime)\n",
    "dataDuringFlooding_na.ResponseTime = dataDuringFlooding_na.CloseDateTime - dataDuringFlooding_na.CallDateTime\n",
    "dataDuringFlooding_na['ResponseTime'] = dataDuringFlooding_na.ResponseTime.dt.total_seconds()\n",
    "dataDuringFlooding_na = dataDuringFlooding_na.set_index('CallDateTime')\n",
    "dataDuringFlooding.update(dataDuringFlooding_na['ResponseTime'])\n",
    "# get the response time during flooding in real world dataset\n",
    "responseDuringFlooding = dataDuringFlooding[['DestinationID', 'ResponseTime']].groupby('DestinationID').mean()\n",
    "# get response time during normal time in real world dataset\n",
    "dataDuringNormal = data.loc[(data.index < '2016-10-08 18:00:00') | (data.index > '2016-10-9 18:00:00')]\n",
    "responseDuringNormal = dataDuringNormal[dataDuringNormal.DestinationID.isin(responseDuringFlooding.index.tolist())][['DestinationID', 'ResponseTime']].groupby('DestinationID').mean()\n",
    "\n",
    "# calculate ratio\n",
    "responseIncrease = responseDuringFlooding.reset_index().merge(responseDuringNormal.reset_index(), on = 'DestinationID', how = 'left', suffixes = ('_flooding', '_normal'))\n",
    "responseIncrease['Ratio'] = responseIncrease['ResponseTime_flooding'] / responseIncrease['ResponseTime_normal']\n",
    "responseIncrease = responseIncrease[~responseIncrease.Ratio.isna()]\n",
    "# add info to roads\n",
    "roads_0 = roads.copy()\n",
    "roads_0 = roads_0[roads_0.OBJECTID.isin(responseIncrease.DestinationID.values.tolist())]\n",
    "roads_0 = roads_0.merge(responseIncrease[['DestinationID', 'Ratio']], how = 'left', left_on = 'OBJECTID', right_on = 'DestinationID')\n",
    "# draw real response time increase ratio\n",
    "# roads_0_select = roads_0[(roads_0.Ratio < 100) & (roads_0.Ratio > 4)]\n",
    "# showRoadsWithValues(roads_0_select, 'Ratio', vmax = roads_0_select.Ratio.max(), vmin = 4, saveAddr = './fig/realResponse_increaseRatio_allPoints', figsize = (30, 30))"
   ],
   "metadata": {
    "id": "XjOrj6ZUc6O1"
   },
   "id": "XjOrj6ZUc6O1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# travel distance increase ratio estimation\n",
    "roads_0 = roads.copy()\n",
    "roads_0 = roads_0[(roads_0.travelTimeIncreaseRatio > 4)]\n",
    "showRoadsWithValues(roads_0, 'travelTimeIncreaseRatio',\n",
    "                    vmax = 10,\n",
    "                    vmin = 4,\n",
    "                    saveAddr = './fig/response_increaseRatio_max10',\n",
    "                    figsize = (30,30))"
   ],
   "metadata": {
    "id": "EktiUFdjih7p"
   },
   "id": "EktiUFdjih7p",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get response time during normal time in real world dataset\n",
    "dataDuringNormal = data.loc[(data.index < '2016-10-08 18:00:00') | (data.index > '2016-10-9 18:00:00')]\n",
    "responseDuringNormal = dataDuringNormal[['DestinationID', 'ResponseTime']].groupby('DestinationID').mean()\n",
    "# calculate prediction of response time\n",
    "roads_0 = roads.copy()\n",
    "roads_0 = roads_0.merge(responseDuringNormal, left_on = 'OBJECTID', right_on = 'DestinationID', how = 'left')\n",
    "roads_0['responseTimePred'] = roads_0.travelTimeIncreaseRatio * roads_0.ResponseTime\n",
    "roads_0 = roads_0[~roads_0.responseTimePred.isna()]\n",
    "roads_0 = roads_0[['OBJECTID', 'responseTimePred']]\n",
    "\n",
    "# fill the nan in dataDuringFlooding\n",
    "dataDuringFlooding = data.loc[(data.index > '2016-10-08 18:00:00') & (data.index < '2016-10-9 18:00:00')]\n",
    "dataDuringFlooding_na = dataDuringFlooding[dataDuringFlooding.ResponseTime.isna()]\n",
    "dataDuringFlooding_na = dataDuringFlooding_na.reset_index()\n",
    "dataDuringFlooding_na.CloseDateTime = pd.to_datetime(dataDuringFlooding_na.CloseDateTime)\n",
    "dataDuringFlooding_na.ResponseTime = dataDuringFlooding_na.CloseDateTime - dataDuringFlooding_na.CallDateTime\n",
    "dataDuringFlooding_na['ResponseTime'] = dataDuringFlooding_na.ResponseTime.dt.total_seconds()\n",
    "dataDuringFlooding_na = dataDuringFlooding_na.set_index('CallDateTime')\n",
    "dataDuringFlooding.update(dataDuringFlooding_na['ResponseTime'])\n",
    "# get the response time during flooding in real world dataset\n",
    "responseDuringFlooding = dataDuringFlooding[['DestinationID', 'ResponseTime']].groupby('DestinationID').mean()\n",
    "# project it on roads\n",
    "roads_1 = roads.copy()\n",
    "roads_1 = roads_1.merge(responseDuringFlooding, left_on = 'OBJECTID', right_on = 'DestinationID', how = 'left')\n",
    "roads_1 = roads_1[~roads_1.ResponseTime.isna()]\n",
    "\n",
    "# combine and organize prediction and true\n",
    "roads_combined = roads_1.merge(roads_0, left_on = 'OBJECTID', right_on = 'OBJECTID', how = 'left')\n",
    "roads_combined = roads_combined[~roads_combined.responseTimePred.isna()]\n",
    "\n",
    "showRoadsWithValues(roads_combined, 'responseTimePred',\n",
    "                    vmax = 1200,\n",
    "                    vmin = 600,\n",
    "                    saveAddr = './fig/responseTime_estimation',\n",
    "                    figsize = (30,30))\n",
    "\n",
    "showRoadsWithValues(roads_combined, 'ResponseTime',\n",
    "                    vmax = 1200,\n",
    "                    vmin = 600,\n",
    "                    saveAddr = './fig/responseTime_real',\n",
    "                    figsize = (30,30))"
   ],
   "metadata": {
    "id": "4HM3k2K_SMr_"
   },
   "id": "4HM3k2K_SMr_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bc827b3a",
   "metadata": {
    "id": "bc827b3a"
   },
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## setup"
   ],
   "metadata": {
    "id": "WkfGrC8EjO_A"
   },
   "id": "WkfGrC8EjO_A"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.basic_variant import BasicVariantGenerator\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import random\n",
    "from darts.models import NaiveSeasonal\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import ccf"
   ],
   "metadata": {
    "id": "bfhTtRQ02VuY"
   },
   "id": "bfhTtRQ02VuY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install statsmodels\n",
    "%pip install pmdarima"
   ],
   "metadata": {
    "id": "kK29xLw7ymgJ"
   },
   "id": "kK29xLw7ymgJ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## preprocess data"
   ],
   "metadata": {
    "id": "7zo1c2_rXQDU"
   },
   "id": "7zo1c2_rXQDU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ccdf9",
   "metadata": {
    "id": "ca9ccdf9"
   },
   "source": [
    "# select data and remove nan value\n",
    "def dataCleaner(data, resStation = None, cutFloodingTime = True, timeIntervalShift = True):\n",
    "\n",
    "    if cutFloodingTime == True:\n",
    "        data = data.loc[:'2016-10-07']\n",
    "\n",
    "    data_simpleLSTM = data[['EnRouteTime',\n",
    "                            'DispatchTime',\n",
    "                            'AssumedRouteLength',\n",
    "                            'AverageSpeed',\n",
    "                            'RescueSquadNumber',\n",
    "                            ]]\n",
    "    if resStation:\n",
    "        data_simpleLSTM = data_simpleLSTM[data['RescueSquadNumber'] == resStation]\n",
    "\n",
    "        ########## NOTE: time interval cal will be WRONG if no res station is selected\n",
    "        data_simpleLSTM['TimeInterval'] = data_simpleLSTM.index.to_series().diff().dt.total_seconds()\n",
    "        data_simpleLSTM = data_simpleLSTM.iloc[1:, :]\n",
    "        if timeIntervalShift == True:\n",
    "            data_simpleLSTM['TimeInterval+1'] = data_simpleLSTM['TimeInterval'].shift(-1)\n",
    "            data_simpleLSTM = data_simpleLSTM.iloc[:-1].drop(['TimeInterval'], axis = 1)\n",
    "\n",
    "    # replace route to inreachale place from nan to a high value\n",
    "    routeLen_mean = data_simpleLSTM.AssumedRouteLength.mean()\n",
    "    routeLen_std = data_simpleLSTM.AssumedRouteLength.std()\n",
    "    data_simpleLSTM['AssumedRouteLength'] = data_simpleLSTM['AssumedRouteLength'].fillna(routeLen_mean + 3 * routeLen_std)\n",
    "\n",
    "    # replace inf or nan speed with mean\n",
    "    speedColumn = data_simpleLSTM.AverageSpeed\n",
    "    speedColumn = speedColumn[~(speedColumn.isin([np.inf, np.nan]))]\n",
    "    speed_mean = speedColumn.mean()\n",
    "    speed_std = speedColumn.std()\n",
    "    data_simpleLSTM['AverageSpeed'] = data_simpleLSTM['AverageSpeed'].replace(np.inf, speed_mean)\n",
    "    data_simpleLSTM['AverageSpeed'] = data_simpleLSTM['AverageSpeed'].fillna(speed_mean)\n",
    "\n",
    "    # fill en route time with mean\n",
    "    data_simpleLSTM['EnRouteTime'] = data_simpleLSTM[['EnRouteTime']].fillna(data_simpleLSTM['EnRouteTime'].mean())\n",
    "\n",
    "    # drop column\n",
    "    data_simpleLSTM = data_simpleLSTM.drop('RescueSquadNumber', axis = 1)\n",
    "\n",
    "    return data_simpleLSTM\n",
    "\n",
    "def sequencesGeneration(data, endIndex, windowSize, startIndex = 0, stride = 1, downSample = 1):\n",
    "    numSequence = (endIndex + 1) - (windowSize - 1)\n",
    "    sequencesIndex = (\n",
    "        startIndex + # start index of the whole data in use\n",
    "        np.tile(np.arange(windowSize, step = downSample), (numSequence, 1)) + # window\n",
    "        np.tile(np.arange(numSequence, step = stride), (windowSize, 1)).T # sequence\n",
    "    )\n",
    "    return data[sequencesIndex]\n",
    "\n",
    "def splitData(sequences, valProportion, testProportion, xCount, yCount, shuffle = False):\n",
    "    if shuffle == True:\n",
    "        np.random.shuffle(sequences)\n",
    "\n",
    "    valCount = int(np.floor(sequences.shape[0] * valProportion))\n",
    "    testCount = int(np.floor(sequences.shape[0] * testProportion))\n",
    "    trainCount = sequences.shape[0] - valCount - testCount\n",
    "\n",
    "    trainX = sequences[:trainCount, :xCount, :]\n",
    "    trainY = sequences[:trainCount, -yCount:, [0]]\n",
    "    valX = sequences[trainCount: trainCount + valCount, :xCount, :]\n",
    "    valY = sequences[trainCount: trainCount + valCount, -yCount:, [0]]\n",
    "    testX = sequences[trainCount + valCount : trainCount + valCount + testCount, :xCount, :]\n",
    "    testY = sequences[trainCount + valCount : trainCount + valCount + testCount, -yCount:, [0]]\n",
    "\n",
    "    print('Dims: (sample, sequenceLen, feature)')\n",
    "    print('trainX shape:', trainX.shape, ' trainY shape:', trainY.shape)\n",
    "    print('valX shape:', valX.shape, ' valY shape:', valY.shape)\n",
    "    print('testX shape:', testX.shape, ' testY shape:', testY.shape)\n",
    "    return trainX, trainY, valX, valY, testX, testY\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## vis"
   ],
   "metadata": {
    "id": "fyqAkCoeBZai"
   },
   "id": "fyqAkCoeBZai"
  },
  {
   "cell_type": "code",
   "source": [
    "# data_select = data_simpleLSTM.iloc[: 70]\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x = data_select.index, y = data_select['DispatchTime']))\n",
    "# fig.add_trace(go.Scatter(x = data_select.index, y = data_select['EnRouteTime']))\n",
    "# fig.show()\n",
    "\n",
    "# data_select = data_simpleLSTM.iloc[: 200]\n",
    "# fig = make_subplots(specs = [[{\"secondary_y\": True}]])\n",
    "# fig.add_trace(go.Scatter(x = data_select.index, y = data_select['EnRouteTime']), secondary_y = False)\n",
    "# fig.add_trace(go.Scatter(x = data_select.index, y = data_select['TimeInterval']), secondary_y = True)\n",
    "# fig.show()"
   ],
   "metadata": {
    "id": "VVXaD40S6wxF"
   },
   "id": "VVXaD40S6wxF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# autocorelation or partical auto-cor plot of prep time\n",
    "plot_pacf(dataCleaner(data, 'R14', cutFloodingTime = True)[['EnRouteTime']])\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "PGAS-b3-Bi1k"
   },
   "id": "PGAS-b3-Bi1k",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# cross correlation plot\n",
    "df = dataCleaner(data, 'R14', cutFloodingTime = True, timeIntervalShift = False)\n",
    "cross_correlation = ccf(df['EnRouteTime'], df['TimeInterval'])\n",
    "# plt.plot(cross_correlation)\n",
    "cross_correlation[:10]"
   ],
   "metadata": {
    "id": "g2FRoCk_C63v"
   },
   "id": "g2FRoCk_C63v",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM"
   ],
   "metadata": {
    "id": "ZRE0ccfTisic"
   },
   "id": "ZRE0ccfTisic"
  },
  {
   "cell_type": "code",
   "source": [
    "# use the data of one station\n",
    "sequenceLen = 15\n",
    "yForward = 1\n",
    "xCount = sequenceLen - yForward\n",
    "data_simpleLSTM = dataCleaner(data, 'R14', cutFloodingTime = True)\n",
    "sequences = sequencesGeneration(data_simpleLSTM.values, data_simpleLSTM.values.shape[0] - 1, 22)\n",
    "trainX, trainY, valX, valY, testX, testY = splitData(sequences, 0.15, 0.15, xCount, yForward, shuffle = False)"
   ],
   "metadata": {
    "id": "8V_jHAq36lWI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686373442766,
     "user_tz": 420,
     "elapsed": 3,
     "user": {
      "displayName": "Xiyu Pan",
      "userId": "05627784278786645916"
     }
    },
    "outputId": "2c42db49-61fc-4dab-e054-ff9d5a37bd77"
   },
   "id": "8V_jHAq36lWI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sequenceLen = 15\n",
    "numFeature = 5\n",
    "yForward = 1\n",
    "xCount = sequenceLen - yForward\n",
    "stationList = ['R14', 'R16', 'R08', 'R18', 'R10', 'R02', 'R09', 'R15', 'R21',\n",
    "               'R04', 'R19', 'R05']\n",
    "sequences = np.empty((1, sequenceLen, numFeature))\n",
    "for station in stationList:\n",
    "    data_simpleLSTM = dataCleaner(data, station, cutFloodingTime = True)\n",
    "\n",
    "    data_simpleLSTM['EnRouteTime'] = data_simpleLSTM.EnRouteTime.clip(upper = data_simpleLSTM.EnRouteTime.mean() + 3 * data_simpleLSTM.EnRouteTime.std())\n",
    "    # data_simpleLSTM['DispatchTime'] = data_simpleLSTM.DispatchTime.clip(upper = data_simpleLSTM.DispatchTime.mean() + 3 * data_simpleLSTM.DispatchTime.std())\n",
    "    data_simpleLSTM['AssumedRouteLength'] = data_simpleLSTM.AssumedRouteLength.clip(upper = data_simpleLSTM.AssumedRouteLength.mean() + 3 * data_simpleLSTM.AssumedRouteLength.std())\n",
    "    data_simpleLSTM['TimeInterval+1'] = data_simpleLSTM['TimeInterval+1'].clip(upper = data_simpleLSTM['TimeInterval+1'].mean() + 3 * data_simpleLSTM['TimeInterval+1'].std())\n",
    "\n",
    "    sequence = sequencesGeneration(data_simpleLSTM.values, data_simpleLSTM.values.shape[0] - 1, sequenceLen)\n",
    "    sequences = np.concatenate([sequences, sequence], axis = 0)\n",
    "sequences = sequences[1:, :, :]\n",
    "\n",
    "# addon = np.tile(sequences[:, -yForward:, -1:], (1, 1, numFeature)) ##### PENDING\n",
    "# sequences_temp = np.concatenate((sequences[:, :-yForward, :], addon), axis = 1) ##### PENDING\n",
    "# sequences = np.concatenate((sequences_temp, sequences[:, -yForward:, :]), axis = 1) ##### PENDING\n",
    "# xCount += 1 ##### PENDING\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY = splitData(sequences, 0.15, 0.15, xCount, yForward)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ng7d3zSEbJsm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686374343041,
     "user_tz": 420,
     "elapsed": 772,
     "user": {
      "displayName": "Xiyu Pan",
      "userId": "05627784278786645916"
     }
    },
    "outputId": "392f6bdb-8005-485f-f487-6f3ce2c68283"
   },
   "id": "Ng7d3zSEbJsm",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def run_LSTM(lr, batch_size, yForward):\n",
    "    model_LSTM = tf.keras.Sequential([\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.LSTM(128),\n",
    "        keras.layers.Dense(yForward),\n",
    "    ])\n",
    "    model_LSTM.compile(optimizer = keras.optimizers.Adam(learning_rate = lr),\n",
    "                loss = \"mse\")\n",
    "    model_LSTM.fit(\n",
    "        x = trainX,\n",
    "        y = trainY,\n",
    "        batch_size = batch_size,\n",
    "        epochs = 500,\n",
    "        validation_data = (valX, valY),\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = \"val_loss\",\n",
    "                min_delta = 0.001,\n",
    "                patience = 3),\n",
    "        ],\n",
    "        verbose = 1,\n",
    "    )\n",
    "    prediction = model_LSTM.predict(testX)\n",
    "    return prediction\n",
    "\n",
    "prediction = run_LSTM(0.001, 512, yForward)"
   ],
   "metadata": {
    "id": "9sQlzhpBm6c_"
   },
   "id": "9sQlzhpBm6c_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class prepTimeDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = np.copy(X)\n",
    "        self.Y = np.copy(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        target = self.Y[idx]\n",
    "        return feature, target\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self, numFeature, yForward):\n",
    "        super().__init__()\n",
    "        self.norm = nn.BatchNorm1d(numFeature)\n",
    "        self.LSTM = nn.LSTM(numFeature, 512, num_layers = 1, batch_first = True)\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, yForward),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.LSTM.flatten_parameters()\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        x, _ = self.LSTM(x)\n",
    "        x = x[:, -1, :]\n",
    "        out = self.net1(x)\n",
    "        return out\n",
    "\n",
    "class earlyStopper():\n",
    "    def __init__(self, patience = 5, minDelta = 0.01):\n",
    "        self.patience = patience\n",
    "        self.minDelta = minDelta\n",
    "        self.counter = 0\n",
    "        self.minValLoss = np.inf\n",
    "\n",
    "    def earlyStop(self, valLoss):\n",
    "        if valLoss < (self.minValLoss - self.minDelta):\n",
    "            self.minValLoss = valLoss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class prepTimePred():\n",
    "    def __init__(self, numFeature, yForward):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        self.model = network(numFeature, yForward)\n",
    "        self.model.to(self.device)\n",
    "        print(self.model)\n",
    "\n",
    "        self.ifTuning = False\n",
    "        self.verbose = 1\n",
    "\n",
    "    def _train(self, dataloader, model, optimizer, lossFunc):\n",
    "        model.train()\n",
    "        size = len(dataloader.dataset)\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(self.device, dtype = torch.float), y.to(self.device, dtype = torch.float)\n",
    "            pred = model(X)\n",
    "            y = y[:, :, 0]\n",
    "            loss = lossFunc(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if batch % 10 == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                if self.verbose == 1:\n",
    "                    print(f\"loss: {loss: >7f}  [{current: >5d} / {size: >5d}]\")\n",
    "\n",
    "    def _val(self, dataloader, model, lossFunc):\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device, dtype = torch.float), y.to(self.device, dtype = torch.float)\n",
    "                pred = model(X)\n",
    "                y = y[:, :, 0]\n",
    "                loss += lossFunc(pred, y).item()\n",
    "        loss /= len(dataloader)\n",
    "        if self.verbose == 1:\n",
    "            print(f\"Avg loss: {loss:>8f} \\n\")\n",
    "        return loss\n",
    "\n",
    "    def learn(self, config, trainX, trainY, valX, valY, epochs = 100):\n",
    "        # load data\n",
    "        trainDataloader = DataLoader(prepTimeDataset(trainX, trainY), batch_size = config[\"batchSize\"])\n",
    "        valDataloader = DataLoader(prepTimeDataset(valX, valY), batch_size = config[\"batchSize\"])\n",
    "        # optimzer and loss\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = config[\"lr\"])\n",
    "        lossFunc = nn.MSELoss()\n",
    "        # learning\n",
    "        earlyStop = earlyStopper(patience = 3, minDelta = 0)\n",
    "        for t in range(epochs):\n",
    "            if self.verbose == 1:\n",
    "                print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            self._train(trainDataloader, self.model, optimizer, lossFunc)\n",
    "            valLoss = self._val(valDataloader, self.model, lossFunc)\n",
    "            if self.ifTuning:\n",
    "                tune.report(valLossFinalIter = valLoss)\n",
    "            if earlyStop.earlyStop(valLoss):\n",
    "                break\n",
    "        if self.verbose == 1:\n",
    "            print(\"Done!\")\n",
    "\n",
    "    def learn4tuning(self, trainX, trainY, valX, valY, numSamples, paramSpace):\n",
    "        self.ifTuning = True\n",
    "        self.verbose = 0\n",
    "        trainable = tune.with_parameters(self.learn, trainX = trainX, trainY = trainY, valX = valX, valY = valY)\n",
    "        trainableWithRes = tune.with_resources(trainable,\n",
    "            resources = lambda x: {\"gpu\": 1} if self.device == 'cuda' else {\"gpu\": 0})\n",
    "        scheduler = ASHAScheduler(\n",
    "            metric = \"valLossFinalIter\",\n",
    "            mode = \"min\",\n",
    "            max_t = 500,\n",
    "            reduction_factor = 2)\n",
    "        tuner = tune.Tuner(\n",
    "            trainableWithRes,\n",
    "            param_space = paramSpace,\n",
    "            tune_config = tune.TuneConfig(\n",
    "                scheduler = scheduler,\n",
    "                search_alg = BasicVariantGenerator(),\n",
    "                num_samples = numSamples,\n",
    "            )\n",
    "        )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        self.ifTuning = False\n",
    "        tuningLog = {result.log_dir: result.metrics_dataframe for result in results}\n",
    "\n",
    "    def predict(self, testX, testY):\n",
    "        self.model.eval()\n",
    "        testDataLoader = DataLoader(prepTimeDataset(testX, testY), batch_size = testX.shape[0])\n",
    "        for batch, (X, y) in enumerate(testDataLoader):\n",
    "            X = X.to(self.device, dtype = torch.float)\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(X)\n",
    "                if self.device == 'cuda':\n",
    "                    return pred.cpu().detach().numpy()\n",
    "                elif self.device == 'cpu':\n",
    "                    return pred.detach().numpy()\n",
    "\n",
    "\n",
    "prepTimePrediction = prepTimePred(numFeature, yForward)\n",
    "prepTimePrediction.learn4tuning(trainX, trainY, valX, valY, 50,\n",
    " {\"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "  \"batchSize\": tune.choice([256, 512, 1024])\n",
    "  })\n",
    "# prepTimePrediction.learn({'lr': 0.0001, 'batchSize': 512}, trainX, trainY, valX, valY)\n",
    "prediction = prepTimePrediction.predict(testX, testY)"
   ],
   "metadata": {
    "id": "Fux3H8ls3zi4"
   },
   "id": "Fux3H8ls3zi4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ranInt = random.randint(0, prediction.shape[0])\n",
    "print(prediction[ranInt])\n",
    "print(testY[ranInt][:,0])\n",
    "print('mean of y', np.mean(testY))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkW30jIQUCyp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686374407501,
     "user_tz": 420,
     "elapsed": 335,
     "user": {
      "displayName": "Xiyu Pan",
      "userId": "05627784278786645916"
     }
    },
    "outputId": "9aa25a5d-31f1-48ee-a84d-df8ceddac3cb"
   },
   "id": "OkW30jIQUCyp",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ARIMA"
   ],
   "metadata": {
    "id": "WK-8zK90iv4p"
   },
   "id": "WK-8zK90iv4p"
  },
  {
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score"
   ],
   "metadata": {
    "id": "-KqPlnj0zzSt"
   },
   "id": "-KqPlnj0zzSt",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_ARIMA = dataCleaner(data, 'R14', cutFloodingTime = True)[['EnRouteTime']]\n",
    "data_ARIMA"
   ],
   "metadata": {
    "id": "NLdMSl2XjDDt"
   },
   "id": "NLdMSl2XjDDt",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_auto = auto_arima(data_ARIMA , trace = True, error_action = 'ignore', suppress_warnings = True)\n",
    "model_auto.fit(data_ARIMA)"
   ],
   "metadata": {
    "id": "BEseMlnPjgVz"
   },
   "id": "BEseMlnPjgVz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = ARIMA(data_ARIMA, order=(3,2,2))\n",
    "model_fit = model.fit()\n",
    "model_fit.predict(start = len(data_ARIMA), end=len(data_ARIMA)+1)\n",
    "mean_squared_error(data_ARIMA.values, model_fit.predict(start = 0, end=4025).values[1:])"
   ],
   "metadata": {
    "id": "VJho-8GI1ZUJ"
   },
   "id": "VJho-8GI1ZUJ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1487292f",
   "metadata": {
    "id": "1487292f"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61ebe3",
   "metadata": {
    "id": "8d61ebe3",
    "scrolled": false
   },
   "source": [
    "plt.style.use('seaborn')\n",
    "fig, ax = plt.subplots(figsize=(30, 7.5))\n",
    "t = range(len(dataTest_y_Flood))\n",
    "ax.plot(t, predictions, label = 'prediction')\n",
    "ax.plot(t, dataTest_y_Flood, label = 'ground truth')\n",
    "ax.set_xlabel('Incident', fontsize = 20)\n",
    "ax.set_ylabel('If accessible', fontsize = 20)\n",
    "ax.legend(fontsize = 20)\n",
    "ax.tick_params(axis='both', labelsize = 15)\n",
    "#ax.set_title('Prediction vs Ground truth', fontsize = 25)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0c3f5",
   "metadata": {
    "id": "00d0c3f5"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "print(classification_report(dataTest_y_Flood, predictions))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57790fa",
   "metadata": {
    "id": "e57790fa",
    "scrolled": false
   },
   "source": [
    "def calculateWaste (row):\n",
    "    if row['If accessible Real'] == 1 and row['If accessible Predicted'] == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculateUnknownDanger (row):\n",
    "    if row['If accessible Real'] == 0 and row['If accessible Predicted'] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dataFlood = dataProcessed.loc['2016-10-09' : '2016-10-09']\n",
    "dataFlood['If accessible Real'] = dataFlood['Accessibility'].astype('int64')\n",
    "dataFlood['If accessible Predicted'] = [element[0] for element in predictions]\n",
    "dataFlood['Waste'] = dataFlood.apply(calculateWaste, axis = 1)\n",
    "dataFlood['Unknown Danger'] = dataFlood.apply(calculateUnknownDanger, axis = 1)\n",
    "dataFlood['Error Type'] = (dataFlood['Waste'] + dataFlood['Unknown Danger'] * 2).astype('string') # 1 mean wastes, 2 means potential danger\n",
    "dataFlood['Error Type'] = dataFlood['Error Type'].replace('1', 'Type 1').replace('2', 'Type 2')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "display(dataFlood)\n",
    "\n",
    "# visualization\n",
    "px.set_mapbox_access_token(open(\"mapboxToken.txt\").read())\n",
    "fig1 = px.scatter_mapbox(dataFlood.loc[lambda df: df['Error Type'] != '0'],\n",
    "                        lat=\"latitude\", lon=\"longitude\",\n",
    "                        color = \"Error Type\", #size = \"Response Time\",\n",
    "                        size_max = 15, zoom = 10, width = 575, height = 500)\n",
    "fig1.show()\n",
    "\n",
    "fig2 = px.scatter_mapbox(dataFlood.loc[lambda df: (df['Error Type'] != '0') &\n",
    "                                      (df['Error Type'] != '1')],\n",
    "                        lat=\"latitude\", lon=\"longitude\",\n",
    "                        color = \"Error Type\", size = \"Response Time\",\n",
    "                        size_max = 30, zoom = 10, width = 550, height = 500)\n",
    "fig2.update_layout(showlegend=False)\n",
    "fig2.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671b767",
   "metadata": {
    "id": "c671b767"
   },
   "source": [
    "# import\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "shp = gpd.read_file('./data/VB_City_Boundary.geojson')\n",
    "shp.crs = 'CRS84'\n",
    "\n",
    "# generate all points\n",
    "numOfPointsOneDimX = 50\n",
    "deltaX = shp.bounds.maxx - shp.bounds.minx\n",
    "deltaY = shp.bounds.maxy - shp.bounds.miny\n",
    "numOfPointsOneDimY = numOfPointsOneDimX * (deltaY / deltaX)\n",
    "\n",
    "xCorList = np.arange(float(shp.bounds.minx), float(shp.bounds.maxx), float(deltaX / numOfPointsOneDimX))\n",
    "yCorList = np.arange(float(shp.bounds.miny), float(shp.bounds.maxy), float(deltaY / numOfPointsOneDimY))\n",
    "xyPointList = [Point(x, y) for x in xCorList for y in yCorList]\n",
    "\n",
    "# select points within the city\n",
    "samplePoints = gpd.GeoSeries(xyPointList)\n",
    "samplePoints.crs = 'CRS84'\n",
    "withinOrNot = samplePoints.within(shp['geometry'].values[0])\n",
    "gdf = pd.concat([samplePoints, withinOrNot], axis = 1)\n",
    "gdf.crs = 'CRS84'\n",
    "gdfSelected = gdf.loc[gdf[1] == True]\n",
    "display(gdfSelected)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7ed19",
   "metadata": {
    "id": "d9b7ed19"
   },
   "source": [
    "# do the prediction for these points\n",
    "gdfSelected['latitude'] = gdfSelected[0].values.y\n",
    "gdfSelected['longitude'] = gdfSelected[0].values.x\n",
    "\n",
    "def addTimeFeature(gdf, hourInDay, dayOfWeek):\n",
    "    gdf_out = gdf.copy()\n",
    "    gdf_out['Hour in Day'] = hourInDay\n",
    "    gdf_out['Day of Week'] = dayOfWeek\n",
    "    return gdf_out\n",
    "\n",
    "gdfSelected_withTime = addTimeFeature(gdfSelected, 0, 6) #The day of week is 1, because it is the normalized value, flooding day is Sunday\n",
    "for hour in range(23):\n",
    "    gdfSelected_withTime = pd.concat([gdfSelected_withTime, addTimeFeature(gdfSelected, hour + 1, 6)])\n",
    "\n",
    "gdfForPrediction = gdfSelected_withTime.reset_index().loc[:,['latitude', 'longitude', 'Hour in Day']]\n",
    "gdfForPrediction_norm = normalizer(gdfForPrediction.copy())\n",
    "gdfForPrediction_norm['Day of Week'] = 1 #The day of week is 1, because it is the normalized value, flooding day is Sunday\n",
    "gdfForPrediction_norm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740f59e",
   "metadata": {
    "id": "3740f59e"
   },
   "source": [
    "predictionsFull = model.predict(gdfForPrediction_norm.values)\n",
    "predictionsFull = np.where(predictionsFull < 0.5, 0, 1).tolist()\n",
    "print(len(predictionsFull))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5cd04",
   "metadata": {
    "id": "18f5cd04"
   },
   "source": [
    "dataFloodFull = gdfForPrediction.copy()\n",
    "dataFloodFull['If accessible'] = [element[0] for element in predictionsFull]\n",
    "dataFloodFull['If accessible'] = dataFloodFull['If accessible'].astype('string').replace('1', 'Accessible').replace('0', 'Inaccessible')\n",
    "display(dataFloodFull)\n",
    "\n",
    "# visualization\n",
    "px.set_mapbox_access_token(open(\"mapboxToken.txt\").read())\n",
    "fig = px.scatter_mapbox(dataFloodFull.loc[dataFloodFull['Hour in Day'] == 23],\n",
    "                        lat = \"latitude\", lon = \"longitude\",\n",
    "                        color = \"If accessible\", #size = \"Response Time\",\n",
    "                        zoom = 9.5, opacity = 0.7, width = 600, height =700)\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8183b",
   "metadata": {
    "id": "90a8183b"
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mA2lYErBQmw2",
    "UJ6D71EgYm1o",
    "e78e5395",
    "bc827b3a",
    "WkfGrC8EjO_A",
    "7zo1c2_rXQDU",
    "fyqAkCoeBZai",
    "ZRE0ccfTisic",
    "WK-8zK90iv4p",
    "1487292f"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
